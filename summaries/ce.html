<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Computer Ethics</title>
  </head>
  <style>
    html {
      scroll-behavior: smooth;
    }
  </style>
  <body>
    <h2>Topics</h2>
    <ul class="topics-list">
      <li>
        <h3>
          Moor, J. (1985). “What is Computer Ethics?”, Metaphilosophy
          16(4):266-275:
        </h3>
      </li>
      <li>
        <h3>
          van de Poel, I., Royakkers, L. (2011). Ethics, Technology, and
          Engineering: An Introduction, Wiley. CHAPTER 1:
        </h3>
      </li>
      <li>
        <h3>
          van de Poel, I., Royakkers, L. (2011). Ethics, Technology, and
          Engineering: An Introduction, Wiley. CHAPTER 3 (from p. 66 to p. 101):
        </h3>
      </li>
      <li>
        <h3>
          van de Poel, I., Royakkers, L. (2011). Ethics, Technology, and
          Engineering: An Introduction, Wiley. CHAPTER 4:
        </h3>
      </li>
      <li>
        <h3>
          Rachels, J. (1975). “Why Privacy is Important”, Philosophy & Public
          Affairs, Vol. 4, No. 4, pp. 323-333:
        </h3>
      </li>
      <li id="privacy_and_information_technology">
        <h3>
          van den Hoven, J., Blaauw, M., Pieters, W. and Warnier, M., "Privacy
          and Information Technology"
          <a href="https://plato.stanford.edu/entries/it-privacy/"
            >The Stanford Encyclopedia of Philosophy (Winter 2014 Edition)</a
          >, Edward N. Zalta (ed.):
        </h3>
      </li>
      <li id="big_data_end_run">
        <h3>
          Barocas, S., Nissenbaum, H., “Big Data’s End Run around Anonymity and
          Consent”, in Privacy, Big Data, and the Public Good Frameworks for
          Engagement”:
        </h3>
        <p>
          This reading basically addresses the issues of Big Data (tracking,
          discrimination...) Defined in the previous reading:
          <a href="#privacy_and_information_technology"
            >Privacy and Information Technology</a
          >.<br />
          Choses a definition of <a href="#big_data">Big Data</a> and
          <a href="#privacy">Privacy</a> to disambiguate. <br />
          Defines briefly the common means that have been used until now to
          protect us from those issues:
          <a href="#anoniminity_and_consent">anoniminity and consent</a>.<br />
          To go later further in depth about them, individually:
          <a href="#anoniminity">Anoniminity</a>,
          <a href="#consent">Consent</a>. <br />
          Showing their limits and fallacies in both theoretical and practical
          scenarios. Arguing that in many contexts they prove to be incapable of
          protecting the individual.
          <br />
        </p>
        <ul>
          <li>
            Anoniminity is rendered almost useless in cases of:
            <a href="#anonymous_identifiers">shared anonymous identifiers</a>,
            <a href="#comprehensiveness">comprehensiveness of the records</a>
            and <a href="#inference">data inference</a>.
          </li>
          <li>
            Consent is irrelevant due to the
            <a href="#transparency_paradox">transparency paradox</a>, the
            <a href="#nature_of_flows">nature of informational flows</a> in the
            big data and the
            <a href="#tiranny_of_the_minority">tiranny of the minority</a>.
          </li>
        </ul>
        <p>
          In conclusion we argue that background and context-driven rights and
          obligations are necessary to guarantee privacy and that Anonimity and
          Consent are useless without them.
        </p>
        <ul>
          <li>
            <h4>Advantages of Big Data</h4>
            <p>
              Big data promises to deliver analytic insights that will add to
              the stock of scientific and social scientific knowledge,
              significantly improve decision makingin boththe public and private
              sector, andgreatly enhanceindividual self-knowledge and
              understanding.
            </p>
          </li>
          <li>
            <h4>Issues of Big Data</h4>
            <p>
              Big data is a threat to fundamental values, including everything
              from autonomy, to fairness, justice, due process, property,
              solidarity, and, perhaps most of all, privacy
            </p>
          </li>
          <li>
            <h4 id="anoniminity_and_consent">
              Anonymity and consent as the solution
            </h4>
            <p>
              anonymization seems to take data outside the scope of privacy, as
              it no longer maps onto identifiable subjects, while allowing
              information subjects to give or withold consent, giving them (in
              theory) control over the information they want to share. This fits
              perfectly the common conception of privacy as: "control over
              information about oneself".
            </p>
          </li>
          <li>
            <h4>Limits of anonymity and consent</h4>
            <p>
              In practice, however, anonymity and consent have proven elusive,
              as time and again critics have revealed fundamental problems in
              implementing both.
            </p>
          </li>
          <li>
            <h4>Virtually intractable challenge to anonymity</h4>
            <p>
              Even when individuals are not ‘identifiable’, they may still be
              ‘reachable’, may still be comprehensibly represented in records
              that detail their attributes and activities, and may be subject to
              consequential inferences and predictions taken on that basis.
            </p>
          </li>
          <li>
            <h4>Virtually intractable challenge to consent</h4>
            <p>
              It is absurd to think that notice and consent can fully specify
              the terms of consent between data collector and data subject. This
              consideration highligts the inefficacy of consent as a matter of
              individual choice.
            </p>
          </li>
          <li id="big_data">
            <h4>Definition of Big Data</h4>
            <p>
              Big Data is a belief in the power of finely observed patterns,
              structures, and models drawn inductively from massive datasets.
            </p>
          </li>
          <li id="privacy">
            <h4>Definition of privacy (as contextual integrity)</h4>
            <p>
              Privacy is the requirement that information about people
              (‘personal information’) flows appropriately, where
              appropriateness means in accordance with
              <a href="#informationsl_norms">informational norms</a>.
            </p>
          </li>
          <li id="informationsl_norms">
            <h4>Informational norms (Theory of contextual integrity)</h4>
            <p>
              Informational norms prescribe information flows according to key
              actors, types of information, and constraints under which flow
              occurs (‘transmission principles’).
            </p>
            <h5>Example</h5>
            <p>
              informational norms for a health care context would govern flow
              between and about people in their context-specific capacities,
              such as physicians, patients, nurses, insurance companies,
              pharmacists, and so forth. Types of information would range over
              relevant fields, including, say, symptoms, diagnoses,
              prescriptions, as well as biographical information. And notable
              among transmission principles, confidentiality is likely to be a
              prominent constraint on the terms under which information types
              flow from, say, patients to physicians.
            </p>
            <h5>Key concept:</h5>
            <p>
              control over information about oneself is not presumed unless the
              other parameters – (context specific) actors and information types
              – warrant it. E.G. If confidentiality is not guaranteed, it must
              not be assumed, hence if violated it does not violate Privacy,
              because information still flows accordingly to informational
              norms.
            </p>
          </li>
          <li>
            <h4>Why privacy as contextual integrity?</h4>
            <p>
              Because computing and information technologies have been radically
              disruptive. To distinguish between technologies that are and are
              not distruptive, a norm-based account of privacy, such as
              contextual integrity, must offer a basis for drawing such
              distinctions.<br />
              By keeping in view connections with specific information flows,
              certain options become salient that might otherwise not have been:
              as soon as we notice a distruption, we can study the flow to see
              what went wrong and caused a disruption.
            </p>
          </li>
          <li id="anoniminity">
            <h4>Anonimity and our assumptions about it</h4>
            <p>
              Anonymity obliterates the link between data and a specific person
              not so much to protect privacy but, in a sense, to bypass it
              entirely. <br />
              If anoniminity can be effectively guaranteed is outside of the
              scope of the extract, we assume that the problem of anonymization,
              classically speaking, has been solved (even if it has obviously
              not). <br />
              Basically, assuming that anoniminity works, is it worth it to
              protect us using it, or is it useless?
            </p>
          </li>
          <li>
            <h4>The value of anoniminity</h4>
            <p>
              The value of anonymity inheres to something we called
              ‘reachability’, the possibility of knocking on your door, holding
              you accountable – with or without access to identifying
              information.
            </p>
          </li>
          <li id="anonymous_identifiers">
            <h4>Anonymous Identifiers</h4>
            <p>
              Unique persistent identifiers that differ from those in common and
              everyday use. They (in theory) avoid association with records of
              other istitutions, but you are still identified within the
              institution that gave you that "Anonymous Identifier". <br />
              In practice any unique identifier or sufficiently unique pattern
              can serve as the basis for recognizing the same person in and
              across multiple databases, so they are not so good. The protective
              value of Unique persistent identifiers decreases as they are
              adopted by or shared with additional institutions (E.G. Social
              Security Number).
            </p>
          </li>
          <li id="comprehensiveness">
            <h4>Comprehensiveness</h4>
            <p>
              A further worry is that the comprehensiveness of the records
              maintained by especially large institutions – records that contain
              no identifying information – may become so rich that they subvert
              the very meaning of anonymity.<br />
              “if a company knows 100 data points about me in the digital
              environment, and that affects how that company treats me in the
              digital world, what’s the difference if they know my name or not?”
              - Turow<br />
              We can no longer turn to anonymity (or, more accurately,
              pseudonymity) to pull datasets outside the remit of privacy
              regulations and debate.
            </p>
          </li>
          <li id="inference">
            <h4>Inference</h4>
            <p>
              Even without names, personal information of Anonymous
              Identifiers,thanks to data mining a lot can be inferred from a
              given big enough data set. This renders the traditional
              protections afforded by anonymity (again, more accurately,
              pseudonymity) much less effective.
            </p>
            <h5>Example</h5>
            <p>
              Rather than attempt to de-anonymize medical records, for instance,
              an attacker (or commercial actor) might instead infer a rule that
              relates a string of more easily observable or accessible
              indicators to a specific medical condition.
            </p>
          </li>
          <li>
            <h4>Research Underwritten by Anonymity</h4>
            <p>
              Findings from the research that they underwrite may provide
              institutions with new paths by which to infer precisely those
              attributes that were previously impossible to associate with
              specific individuals in the absence of identifying information.
              <br />
              Basically a research meant to guarantee Anoniminty enables
              institutions to infer characteristics that were previously
              "protected" by anoniminity. They should have discussed about that
              beforehand.
            </p>
          </li>
          <li id="consent">
            <h4>Informed consent</h4>
            <p>
              Where anonymity is unachievable or simply does not make sense,
              informed consent often is the mechanism sought out by
              conscientious collectors and users of personal information.
            </p>
          </li>
          <li>
            <h4>Fair Information Practice Principles (FIPPs)</h4>
            <p>
              Theese principles of informed consent in privacy law. These
              principles, in broad brushstrokes, demand that data subjects be
              given notice, that is to say, informed who is collecting, what is
              being collected, how information is being used and shared, and
              whether information collection is voluntary or required. The
              Internet challenged them a lot, eventually coming up with privacy
              policies.
            </p>
          </li>
          <li>
            <h4>Privacy Policies (How consent is handled in the Internet)</h4>
            <p>
              Over the course of roughly a decade and a half, privacy policies
              have remained the linchpin of privacy protection online, despite
              overwhelming evidence that most of us neither read nor understand
              them. The debate on how to make them better is still active (opt
              in vs opt out, ...).
            </p>
          </li>
          <li>
            <h4>Assumptions about consent</h4>
            <p>
              We accept that informed consent is a useful privacy measure in
              certain circumstances and against certain threats and that
              existing mechanisms can and should be improved, but, against the
              challenges of big data, consent, by itself, has little traction.
              (much like the approach taken on
              <a href="#anoniminity">Anonimity</a>)
            </p>
          </li>
          <li id="transparency_paradox">
            <h4>The Transparency Paradox</h4>
            <p>
              Simplicity and clarity unavoidably results in losses of fidelity.
              <br />
              A few simple and clear statments cannot specify all the
              implications of consenting. <br />Even the few people want to go
              in detail and actually read the policies, will most likely not
              understand them. <br />And even if they do understand, the
              information given to them will be so general that they still will
              not – indeed cannot – be informed in ways relevant to their
              decisions whether to consent.
            </p>
            <h5>Example</h5>
            <p>
              What can it mean to an ordinary person that the information will
              be shared with Axciom or Choicepoint, let alone the NSA? <br />
              Characterizing the type of information is even tougher. Is it
              sufficient for the utility company to inform customers that it is
              collecting smart meter readings?
            </p>
          </li>
          <li id="nature_of_flows">
            <h4>Indeterminate, Unending, Unpredictable data flows and usage</h4>
            <p>
              Deciding how to describe information practices in ways that are
              relevant to privacy so that individuals meaningfully grant or
              withhold consent is difficult due to the machinations of big data:
              data moves from place to place and recipient to recipient in
              unpredictable ways. <br />
              Who will ask to buy that data (if at all)? <br />
              To which data will this data be aggregated?. <br />
              Basically, the flows of the
              <a href="informational_norms">informational noms</a> are not well
              defined unless recipients and transmission principles are
              specified (which is almost never the case). <br />
              As many have now argued, consent under those conditions is not
              meaningful.
            </p>
          </li>
          <li>
            <h4>How to consent for correlated data?</h4>
            <p>
              What if from a large dataset, using data mining techniques, some
              unanticipated correlations are discovered? <br />Is it the data
              controller's responsibility to warn the informing subjects about
              such possibility? <br />How should he inform them, if even he does
              not know the possible findings in advance? <br />
              As many have now argued, consent under those conditions is not
              meaningful.
            </p>
          </li>
          <li id="tiranny_of_the_minority">
            <h4>The tiranny of the minority</h4>
            <p>
              The volunteered information of the few can unlock the same
              information about the many. <br />
              "it’s no longer about what you do that will go down on your
              permanent record. Everything that everyone else does that concerns
              you, implicates you, or might influence you will go down on your
              permanent record."" - Hence danah. <br />
              It is even possible to make inferences about people who are not
              even a part of an online social network (i.e. to learn things
              about obviously absent nonmembers). <br />
            </p>
            <h5>Examples</h5>
            <p>
              1) Let's say that 10 people near to you consent to share their
              data, from that data a certain pattern is extracted E.G. those
              people switch from WhatsApp to Telegram. It is now possible to
              infer that you are likely switch from WhatsApp to Telegram.<br />
              2) More generally: Target was able to infer a rule about the
              relationship between purchases and pregnancy from what must have
              been a tiny proportion of all its customers who actually decided
              to tell the company that they recently had a baby.
            </p>
            <h5>Consequences</h5>
            <p>
              Multiple attributes can be inferred globally when as few as 20% of
              the users reveal their attribute information. <br />
              Once a critical threshold has been reached, data collectors can
              rely on more easily observable information to situate all
              individuals according to these patterns, rendering irrelevant
              whether or not those individuals have consented to allowing access
              to the critical information in question. Withholding consent will
              make no difference to how they are treated!
            </p>
          </li>
          <li>
            <h4>Points of view</h4>
            <p>
              1) Some people think that privacy and big data are incompatible,
              to them, the points shown before just enforce the need to dislodge
              privacy from its pedestal and allowthe glorious potential of big
              data to be fulfilled. <br />
              2) Others say that we should remain concerned about ethical issues
              raised by big data, that, while privacy may be a lost cause, the
              real problems arise with use. Basiccally allow data collection and
              prevent misuse. <br />
              3) Our point of view: we are not yet readyto give up on privacy,
              nor completely on anonymity and consent.
            </p>
          </li>
          <li>
            <h4>Our conclusions:</h4>
            <p>
              Background and context-driven rights and obligations have been
              neglected in favor of anonymity and consent to the detriment of
              individuals and social integrity. They should instead be the
              foundation of how privacy is guaranteed: <br />
              0) There should be a set of background and context driven rights
              and obligations that must be respected. <br />
              1) The user should only consent to data collection or sharing that
              violates this set of obligations <br />
              2) This sharing should be very limited in time and purpose <br />
              3) It is up to the data user to justify why certain data shall be
              collected. <br />
              4) Where, for example, anonymizing data, adopting pseudonyms, or
              granting or withholding consent makes no difference to outcomes
              for an individual, we had better be sure that the outcomes in
              question can be defended as morally and politically legitimate.
              <br />
              5) When anonymity and consent do make a difference, we learn from
              the domain of scientific integrity that simply because someone is
              anonymous or pseudonymous or has consented does not by itself
              legitimate the action in question.
              <br />
            </p>
          </li>
        </ul>
      </li>
      <li>
        <h3>
          Stallman, R. (1995). “Why software should be free”. Computer Ethics
          and Social Values, Johnson, D. and Nissenbaum, H. (eds.), Prentice
          Hall, 190-199:
        </h3>
      </li>
    </ul>
    <h3>Exam Dates</h3>
    <ul class="types-list">
      <li>21 DEC</li>
      <li>22 JAN</li>
      <li>15 FEB</li>
      <!-- Add more exam dates as needed -->
    </ul>
  </body>
</html>
