<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Computer Ethics</title>
  </head>
  <style>
    html {
      font-family: Arial, sans-serif;
      scroll-behavior: smooth;
      font-size: 2em;
      margin: 1em;
    }

    section.minimized ul {
      display: none;
    }
    section.minimized p {
      display: none;
    }
    section.compressed li p {
      display: none;
    }
    section.compressed li h5 {
      display: none;
    }
    section.compressed li ul {
      display: none;
    }
    section.compressed table {
      display: none;
    }
  </style>

  <body>
    <h1>Computer Ethics</h1>
    <h2 onclick="toggleCompress()">Topics, click to show</h2>
    <section>
      <h3>
        Moor, J. (1985). “What is Computer Ethics?”, Metaphilosophy
        16(4):266-275:
      </h3>
      <ul>
        <li>
          <h4>What is computer ethics</h4>
          <p>
            computer ethics is the analysis of the nature and social impact of
            computer technology and the corresponding formulation and
            justification of policies for the ethical use of such technology.
          </p>
        </li>
        <li>
          <h4>Inadequacy of policies</h4>
          <p>
            Computers provide us with new capabilities and these in turn give us
            new choices for action. Often, either no policies for conduct in
            these situations exist or existing policies seem inadequate. <br />
            Computer ethics includes consideration of both personal and social
            policies for the ethical use of computer technology.
          </p>
        </li>
        <li>
          <h4>Conceptual vacuum</h4>
          <p>
            A deeper analisis reveals that the Inadequacy of policy is due to a
            messy conceptual framework. <br />
            Hence much of the important work in computer ethics is devoted to
            proposing conceptual frameworks for understanding ethical problems
            involving computer technology. <br />
            It is important to give the instruments to understand ethical
            problems, not just the solutions (if any).
          </p>
        </li>
        <li>
          <h4>Hazards of choosing a conceptualization</h4>
          <p>
            The conceptualization we pick will not only affect how a policy will
            be applied but to a certain extent what the facts are. <br />
            Even within a coherent conceptual framework, the formulation of a
            policy for using computer technology can be difficult.
          </p>
        </li>
        <li>
          <h4>The discipline of computer ethics</h4>
          <p>
            Although computer ethics is a field between science and ethics and
            depends on them, it is also a discipline in its own right which
            provides both conceptualizations for understanding and policies for
            using computer technology.
          </p>
        </li>
        <li>
          <h4>What is special about computers</h4>
          <p>
            Mere abundance and affordability don't seem sufficient to justify
            any claim to technological revolution. <br />
            Small, fast, powerful and easy-to-use electric can openers are great
            improvements over earlier can openers, but they aren't in the
            relevant sense revolutionary. <br />
            The real reason is
            <a href="#logical_malleability">logical malleability</a>
          </p>
        </li>
        <li>
          <h4>Logical malleability</h4>
          <p>
            Computers are logically malleable in that they can be shaped and
            molded to do any activity that can be characterized in terms of
            inputs, outputs, and connecting logical operations. <br />
            The computer is the nearest thing we have to a universal tool.
          </p>
        </li>
        <li>
          <h4>Not just number crunchers</h4>
          <p>
            There is a popular opinion whhich sees computer essentially as
            numerical devices. <br />
            Computers manipulate symbols but they don't care what the symbols
            represent. <br />
            but we humans can assign any meaning to those symbols, hence we
            can't restrict them to being number crunchers.
          </p>
        </li>
        <li>
          <h4>Stages of computer revolution</h4>
          <p>
            Hypothesis: it will behave similarly to the industrial revolution.
            <br />
            It will have 2 stages:
            <a href="#introduction">introduction</a> and
            <a href="#permeation">permeation</a>
          </p>
        </li>
        <li>
          <h4 id="introduction">Introduction stage</h4>
          <p>
            It has been occurring during the last forty years. Electronic
            computers have been created and refined. <br />
            During the introduction stage computers are understood as tools for
            doing standard jobs.
          </p>
        </li>
        <li>
          <h4 id="permeation">Permeation stage</h4>
          <p>
            We are entering it. Computer technology will become an integral part
            of institutions throughout our society. <br />
            during the permeation stage, computers become an integral part of
            the activity.
          </p>
          <h5>Example</h5>
          <p>
            Computer have been used for counting votes instantly (and making the
            results public) raising an important question: <br />
            Is it appropriate that some people know the outcome before they
            vote? <br />
            The problem is that computers not only tabulate the votes for each
            candidate but likely influence the number and distribution of these
            votes.
          </p>
        </li>
        <li>
          <h4>Future questions</h4>
          <p>
            Not "How well do computers help us work?" but "What is the nature of
            this work?"<br />
            Not "How well do computers count money?" but "What is money?" <br />
            Not "How well do computers educate?" but "What is education?"<br />
          </p>
        </li>
        <li>
          <h4>The importance of computer ethics</h4>
          <p>
            The revolutionary feature of computers is their logical
            malleability. Logical malleability assures the enormous application
            of computer technology. This will bring about the Computer
            Revolution. During the Computer Revolution many of our human
            activities and social institutions will be transformed. These
            transformations will leave us with policy and conceptual vacuums
            about how to use computer technology. Such policy and conceptual
            vacuums are the marks of basic problems within computer ethics.
            Therefore, computer ethics is a field of substantial practical
            importance.
          </p>
        </li>
        <li>
          <h4>Invisibility factor</h4>
          <p>
            Most of the time and under most conditions computer operations are
            invisible.
          </p>
        </li>
        <li>
          <h4>Invisible abuse</h4>
          <p>
            Is the intentional use of the invisible operations of a computer to
            engage in unethical conduct.
          </p>
          <h5>Examples</h5>
          <p>
            A programmer who realized he could steal excess interest from a
            bank. <br />
            The invasion of the property and privacy of others. <br />
            The use of computers for surveillance.
          </p>
        </li>
        <li>
          <h4>Invisible programming values</h4>
          <p>
            Those values which are embedded in a computer program. <br />
            In order to implement a program which satisfies the specifications a
            programmer makes some value judgments about what is important and
            what is not. These values become embedded in the final product and
            may be invisible to someone who runs the program. <br />
            Even if one sets out to create a program for a completely unbiased
            reservation service, some value judgments are latent in the program
            because some choices have to be made about how the program operates.
            <br />
            Sometimes invisible programming values are so invisible that even
            the programmers are unaware of them. Programs may have bugs or may
            be based on implicit assumptions which don't become obvious until
            there is a crisis.
          </p>
          <h5>Example</h5>
          <p>
            American Airlines once promoted such a service called "SABRE". This
            program had a bias for American Airline flights built in so that
            sometimes an American Airline flight was suggested by the computer
            even if it was not the best flight available. Indeed, Braniff
            Airlines, which went into bankruptcy for awhile, sued American
            Airlines on the grounds that this kind of bias in the reservation
            service contributed to its financial difficulties.
          </p>
        </li>
        <li>
          <h4>Invisible complex calculation</h4>
          <p>
            Computers today are capable of enormous calculations beyond human
            comprehension. Even if a program is understood, it does not follow
            that the calculations based on that program are understood.
          </p>
          <h5>Example</h5>
          <p>
            After more than a thousand hours of computer time on various
            computers, the four color conjecture was proved correct. What is
            interesting about this mathematical proof, compared to traditional
            proofs, is that it is largely invisible. The general structure of
            the proof is known and found in the program and any particular part
            of the computer's activity can be examined, but practically speaking
            the calculations are too enormous for humans to examine them all.
          </p>
        </li>
        <li>
          <h4>A possible solution to the invisibility factor</h4>
          <p>
            Computers can make the invisible visible. Information which is lost
            in a sea of data can be clearly revealed with the proper computer
            analysis.<br />
            We don't want to inspect every computerized transaction or program
            every step for ourselves or watch every computer calculation. In
            terms of efficiency the invisibility factor is a blessing. <br />
            The challenge for computer ethics is to formulate policies which
            will help us deal with this dilemma. We must decide when to trust
            computers and when not to trust them.
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>
        van de Poel, I., Royakkers, L. (2011). Ethics, Technology, and
        Engineering: An Introduction, Wiley. CHAPTER 1:
      </h3>
      <ul>
        <li>
          <h4>Responsibility</h4>
          <p>
            Responsibility means in the first place being held accountable for
            your actions and for the effects of your actions. The making of
            choices, the taking of decisions but also failing to act are all
            things that we regard as types of actions.
          </p>
        </li>
        <li>
          <h4>Role responsibility</h4>
          <p>
            The responsibility that is based on the role one has or plays in a
            certain situation. <br />
            Roles and their accompanying responsibilities can be formally laid
            down, for instance legally, in a contract or in professional or
            corporate codes of conduct. <br />
            Since a person often has different roles in life he/she has various
            role responsibilities.
          </p>
          <h5>Example</h5>
          <p>
            Boisjoly for example in the Challenger case both had a role as an
            employee and as an engineer. As an employee he was expected to be
            loyal to his company and to listen to his superiors, who eventually
            decided to give positive advice about the launch. As an engineer he
            was expected to give technically sound advice taking into account
            the possible risks to the astronauts and, in his view, this implied
            a negative advice with respect to the launch.
          </p>
        </li>
        <li>
          <h4>Moral responsibility</h4>
          <p>
            Responsibility that is based on moral obligations,
            <a href="#morality">moral</a> norms or
            <a href="#morality">moral</a> duties.<br />
            It is not confined to the roles one plays in a situation.<br />
            Moral responsibility can, however, also limit role responsibilities
            because with some roles immoral responsibilities may be associated.
          </p>
          <h5>Example</h5>
          <p>
            In the Challenger case, it was part of Boisjoly’s moral
            responsibility to care for the consequences of his advice for the
            astronauts and for others.
          </p>
        </li>
        <li>
          <h4>Professional responsibility</h4>
          <p>
            Professional responsibility is the responsibility that is based on
            your role as a professional engineer in as far it stays within the
            limits of what is morally allowed.
          </p>
        </li>
        <li>
          <h4>Passive responsibility</h4>
          <p>
            Backward-looking responsibility, relevant after something
            undesirable occurred. <br />
            Specific forms are
            <a href="#accountability">accountability</a>,
            <a href="#blameworthiness">blameworthiness</a>, and
            <a href="#liability">liability</a>
          </p>
        </li>
        <li>
          <h4 id="accountability">Accountability</h4>
          <p>
            Backward-looking responsibility in the sense of being held to
            account for, or justify one’s actions towards others.
          </p>
        </li>
        <li>
          <h4 id="blameworthiness">Blameworthiness</h4>
          <p>
            Backward-looking responsibility in the sense of being a proper
            target of blame for one’s actions or the consequences of one’s
            actions. In order for someone to be blameworthy, usually the
            following conditions need to apply:
            <a href="#wrong_doing">wrong-doing</a>,
            <a href="#causal_contribution">causal contribution</a>,
            <a href="#foreseeability">foreseeability</a>
            <a href="#freedom"></a>.
          </p>
        </li>
        <li>
          <h4 id="wrong_doing">Wrong-doing</h4>
          <p>
            In carrying out a certain action the individual or the institution
            in question has violated a norm or did something wrong.
          </p>
        </li>
        <li>
          <h4 id="causal_contribution">Causal contribution</h4>
          <p>
            The person who is held responsible must have made a causal
            contribution to the consequences for which he or she is held
            responsible. <br />
            1) Not only an action, but also a failure to act may often be
            considered a causal contribution. <br />
            2) A causal contribution is usually not a sufficient condition for
            the occurrence of the consequence under consideration.
          </p>
        </li>
        <li>
          <h4 id="foreseeability">Foreseeability</h4>
          <p>
            A person who is held responsible for something must have been able
            to know the consequences of his or her actions. <br />
            People cannot be held responsible if it is totally unreasonable to
            expect that they could possibly have been aware of the consequences.
          </p>
        </li>
        <li>
          <h4 id="freedom">Freedom of action</h4>
          <p>
            The one who is held responsible must have had freedom of action,
            that is, he or she must not have acted under compulsion. Individuals
            are either not responsible or are responsible to a lesser degree if
            they are, for instance, coerced to take certain decisions.
          </p>
        </li>
        <li>
          <h4>Active responsibility</h4>
          <p>
            Responsibility before something has happened referring to a duty or
            task to care for certain state-of-affairs or persons. <br />
            If someone is actively responsible for something he/she is expected
            to act in such a way that undesired consequences are avoided as much
            as possible and so that positive consequences are realized.
          </p>
        </li>
        <li>
          <h4>Mark Bovens'features of active responsibility</h4>
          <p>
            1) Adequate perception of threatened violations of norms; <br />
            2) Consideration of the consequences; <br />
            3) Autonomy, i.e. the ability to make one’s own independent moral
            decisions; <br />
            4) Displaying conduct that is based on a verifiable and consistent
            code; <br />
            5) Taking role obligations seriously. (Bovens, 1998)
          </p>
        </li>
        <li>
          <h4>Ideals</h4>
          <p>
            Ideas or strivings which are particularly motivating and inspiring
            for the person having them, and which aim at achieving an optimum or
            maximum.
          </p>
        </li>
        <li>
          <h4>Professional ideals</h4>
          <p>
            Ideals that are closely allied to a profession or can only be
            aspired to by carrying out the profession.
          </p>
          <h5>Example</h5>
          <p>
            Ideals of engineers are directly linked to professional practice
            because they are closely allied to the engineering profession or can
            only be aspired to by carrying out the profession of engineer.
          </p>
        </li>
        <li>
          <h4>Technological enthusiasm</h4>
          <p>
            Ideal of wanting to develop new technological possibilities and take
            up technological challenges.
          </p>
          <h5>Danger</h5>
          <p>
            The inherent danger of technological enthusiasm lies in the possible
            negative effects of technology and the relevant social constraints
            being easily overlooked. (e.g. Rockets used for bombing instead of
            travel).
          </p>
        </li>
        <li>
          <h4>Effectiveness and efficiency</h4>
          <p>
            1) Effectiveness: The extent to which an established goal is
            achieved. <br />
            2) Efficiency The ratio between the goal achieved and the effort
            required.
          </p>
          <h5>Danger</h5>
          <p>
            The matter of whether effectiveness or efficiency is morally worth
            pursuing therefore depends very much on the ends for which they are
            employed. <br />
            So, although some engineers have maintained the opposite, the
            measurement of the effectiveness and efficiency of a technology is
            value-laden. <br />
            1) Goal can be value-laden. (e.g. efficiency in sterminating Jews
            (Jews/gas)) <br />
            2) Input may be value-laden. (e.g. a technology may be efficient in
            terms of costs but not in terms of energy consumption)
          </p>
        </li>
        <li>
          <h4>Human welfare</h4>
          <p>Ideal of contributing to or augmenting human welfare.</p>
        </li>
        <li>
          <h4>
            Technological enthusiasm, effectiveness and efficiency VS human
            welfare
          </h4>
          <p>
            As we have seen technological enthusiasm and effectiveness and
            efficiency are morally neutral; in both cases much depends on the
            goals for which technology is used and the side-effects so created.
            <br />
            Human welfare confirms that the professional practice of engineers
            is not something that is morally neutral and that engineers do more
            than merely develop neutral means for the goals of others.
          </p>
        </li>
        <li>
          <h4>Engineers versus Managers</h4>
          <p>
            Engineers are often salaried employees and they are usually
            hierarchically below managers. <br />
            There are three models of dealing with this tension and the
            potential conflict between engineers and managers:
            <a href="#separatism">separatism</a>,
            <a href="#technocracy">technocracy</a>, and
            <a href="#whistle_blowing">whistle-blowing</a>.
          </p>
        </li>
        <li>
          <h4 id="separatism">Separatism</h4>
          <p>
            The notion that scientists and engineers should apply the technical
            inputs, but appropriate management and political organs should make
            the value decisions.
          </p>
        </li>
        <li>
          <h4>Tripartite model</h4>
          <p>
            A model that maintains that engineers can only be held responsible
            for the design of products and not for wider social consequences or
            concerns. <br />
            In the tripartite model three separate segments are distinguished:
            the segment of politicians, managers, principals and (anticipated)
            customers; the segment of engineers; and the segment of users.
          </p>
        </li>
        <li>
          <h4>Hired gun</h4>
          <p>
            Someone who is willing to carry out any task or assignment from his
            employer without moral scruples.
          </p>
          <h5>Song</h5>
          <p>
            Once the rockets go up <br />
            Who cares where they come down <br />
            “that’s not my department,” <br />
            said Wernher von Braun.
          </p>
        </li>
        <li>
          <h4 id="technocracy">Technocracy</h4>
          <p>Government by experts.</p>
          <h5>Issues</h5>
          <p>
            1) It is not exactly clear what unique expertise engineers possess
            that permit them to legitimately lay claim to the role of
            technocrats. <br />
            2) Technocracy is undemocratic and
            <a href="#paternalistic">paternalistic</a>.
          </p>
        </li>
        <li>
          <h4 id="paternalism">Paternalism</h4>
          <p>
            The making of (moral) decisions for others on the assumption that
            one knows better what is good for them than those others themselves.
          </p>
        </li>
        <li>
          <h4 id="whistle_blowing">Whistle-blowing</h4>
          <p>
            The disclosure of certain abuses in a company by an employee in
            which he or she is employed, without the consent of his/ her
            superiors, and in order to remedy these abuses and/or to warn the
            public about these abuses.
          </p>
          <h5>Issues</h5>
          <p>
            1) Whistle-blowing usually forces people to make big sacrifices and
            one may question whether it is legitimate to expect the average
            professional to make such sacrifices. <br />
            2) The effectiveness of whistle-blowing is often limited because as
            soon as the whistle is blown the communication between managers and
            professionals has inevitably been disrupted.
          </p>
        </li>
        <li>
          <h4>Guidelines for Whistle-Blowing</h4>
          <p>
            1) The organization to which the would-be whistleblower belongs
            will, through its product or policy, do serious and considerable
            harm to the public (whether to users of its product, to innocent
            bystanders, or to the public at large). <br />
            2) The would-be whistleblower has identified that threat of harm,
            reported it to her immediate superior, making clear both the threat
            itself and the objection to it, and concluded that the superior will
            do nothing effective. <br />
            3) The would-be whistleblower has exhausted other internal
            procedures within the organization (for example, by going up the
            organizational ladder as far as allowed) – or at least made use of
            as many internal procedures as the danger to others and her own
            safety make reasonable. <br />
            4) The would-be whistleblower has (or has accessible) evidence that
            would convince a reasonable, impartial observer that her view of the
            threat is correct. <br />
            5) The would-be whistleblower has good reason to believe that
            revealing the threat will (probably) prevent the harm at reasonable
            cost (all things considered). (De George, 1990)
          </p>
        </li>
        <li>
          <h4>Social Context of Technological Development</h4>
          <p>
            1) Engineers are just one of the many actors involved in technology
            development and cannot alone determine technological development and
            its social consequences, not all responsibility is theirs. <br />
            2) Engineers have to take into account a range of stakeholders and
            their interests. They cannot, just as technocrats, decide in
            isolation what the right thing to do is, but they need to involve
            other stakeholders in technological development and to engage in
            discussions with them.
          </p>
        </li>
        <li>
          <h4>Actor</h4>
          <p>
            Any person or group that can make a decision how to act and that can
            act on that decision. <br />
            All actors have certain interests: things actors strive for because
            they are beneficial or advantageous for them.
          </p>
        </li>
        <li>
          <h4>Developers and producers of technology.</h4>
        </li>
        <li>
          <h4>Users</h4>
          <p>
            People who use a technology and who may formulate certain wishes or
            requirements for the functioning of a technology.
          </p>
        </li>
        <li>
          <h4>Regulators</h4>
          <p>
            Organizations who formulate rules or regulations that engineering
            products have to meet such as rulings concerning health and safety,
            but also rulings linked to relations between competitors.
          </p>
        </li>
        <li>
          <h4>Stakeholders</h4>
          <p>
            Actors that have an interest (“a stake”) in the development of a
            technology, but who cannot necessarily influence the direction of
            technological development (e.g. people living in the vicinity of a
            planned construction site for a nuclear plant.). We should think
            about them too, even if they are powerless.
          </p>
        </li>
        <li>
          <h4>Technology Assessment (TA)</h4>
          <p>
            Systematic method for exploring future technology developments and
            assessing their potential societal consequences.
          </p>
        </li>
        <li>
          <h4>Collingridge dilemma</h4>
          <p>
            This dilemma refers to a double-bind problem to control the
            direction of technological development. On the one hand, it is often
            not possible to predict the consequences of new technologies already
            in the early phases of technological development. On the other hand,
            once the (negative) consequences materialize it often has become
            very difficult to change the direction of technological development.
          </p>
        </li>
        <li>
          <h4>Constructive Technology Assessment (CTA)</h4>
          <p>
            Approach to Technology Assessment (TA) in which TA-like efforts are
            carried out parallel to the process of technological development and
            are fed back to the development and design process. <br />
            Among other things, this implies that stakeholders get a larger say
            in technological development!
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>
        van de Poel, I., Royakkers, L. (2011). Ethics, Technology, and
        Engineering: An Introduction, Wiley. CHAPTER 3 (from p. 66 to p. 101):
      </h3>
      <ul>
        <li>
          <h4>Ethical theories</h4>
          <p>
            The role of ethical theories is to provide certain arguments or
            reasons for a moral judgment. They provide a normative framework for
            understanding and responding to moral problems, so improving ethical
            decision-making or, at least, avoiding certain shortcuts, such as
            neglecting certain relevant features of the problem or just stating
            an opinion without any justification.
          </p>
        </li>
        <li>
          <h4>Ethics</h4>
          <p>
            The the systematic reflection on what is moral. <br />
            Ethics is a process of searching for the right kind of morality.
          </p>
        </li>
        <li>
          <h4 id="morality">Morality</h4>
          <p>
            The totality of opinions, decisions, and actions with which people
            express, individually or collectively, what they think is good or
            right. <br />
            1) Moral <a href="#values">values</a> help us determine which goals
            or states of affairs are worth striving for in life, to lead a good
            life or to realize a just society.<br />
            2) Moral <a href="#norms">norms</a> are rules that prescribe what
            action is required, permitted, or forbidden.<br />
            3) Moral <a href="#virtues">virtues</a> are character traits that
            make someone a good person or that allow people to lead good
            lives.<br />
          </p>
        </li>
        <li>
          <h4>Descriptive ethics</h4>
          <p>
            The branch of ethics that describes existing morality, including
            customs and habits, opinions about good and evil, responsible and
            irresponsible behaviour, and acceptable and unacceptable action.
          </p>
        </li>
        <li>
          <h4>Normative (prescriptive) ethics</h4>
          <p>
            The branch of ethics that judges morality and tries to formulate
            normative recommendations about how to act or live. <br />
            Normative ethics is not value-free; it judges morality. It considers
            the following main question: do the norms and values actually used
            conform to our ideas about how people should behave? Normative
            ethics does not give an unambiguous answer to this question, but in
            its moral judgment various arguments are given based on various
            ethical theories.
          </p>
        </li>
        <li>
          <h4>Descriptive judgments</h4>
          <p>
            A judgment that describes what is actually the case (the present),
            what was the case (the past), or what will be the case (the future).
            Descriptive judgments are true or false. <br />
            Sometimes the truth of a descriptive statement has not yet been
            determined because testing is impossible.
          </p>
          <h5>Example</h5>
          <p>
            The assertion “the Challenger met all safety standards of the time”
            is a descriptive judgment: the assertion is true or false.
          </p>
        </li>
        <li>
          <h4>Normative judgment</h4>
          <p>
            Judgment about whether something is good or bad, desirable or
            undesirable, right or wrong.
          </p>
          <h5>Examples</h5>
          <p>
            “the Challenger should never have been launched,” “Engineers should
            faithfully provide measurements.”, and “stealing is bad.”
          </p>
        </li>
        <li>
          <h4 id="values">Values</h4>
          <p>
            Lasting convictions or matters that people feel should be strived
            for in general and not just for themselves to be able to lead a good
            life or to realize a just society.
          </p>
        </li>
        <li>
          <h4>Intrinsic value</h4>
          <p>
            Value in and of itself. (e.g. The value of money for Scrooge McDuck)
          </p>
        </li>
        <li>
          <h4>Instrumental value</h4>
          <p>
            Something that is valuable in as far as it is a means to, or
            contributes to something else that is intrinsically good or
            valuable. (e.g. The value of money for For Mother Theresa)
          </p>
        </li>
        <li>
          <h4 id="norms">Norms</h4>
          <p>
            Rules that prescribe what concrete actions are required, permitted
            or forbidden. These are rules and agreements about how people are
            supposed to treat each other.
          </p>
        </li>
        <li>
          <h4 id="virtues">Virtues</h4>
          <p>
            a certain type of human characteristics or qualities that has the
            following five features: <br />
            1) They are desired characteristics and they express a value that is
            worth striving for. <br />
            2) They are expressed in action. <br />
            3) They are lasting and permanent – they form a lasting structural
            foundation for action. <br />
            4) They are always present, but are only used when necessary. <br />
            5) They can be influenced by the individual (MacIntyre, 1984a).
            <br />
            NB: People can learn virtues!
          </p>
        </li>
        <li>
          <h4>Intellectual virtues</h4>
          <p>Focus on knowledge and skills.</p>
        </li>
        <li>
          <h4>Moral virtues</h4>
          <p>
            Are the desirable characteristics of people – the characteristics
            that make people good.
          </p>
        </li>
        <li>
          <h4>Normative relativism</h4>
          <p>
            An ethical theory that argues that all moral points of view – all
            values, norms, and virtues – are equally valid.
          </p>
          <h5>Issue</h5>
          <p>
            A system that allowed engineers to disregard these rules based on
            his or her personal values (which other people have to respect)
            would create an unworkable situation.
          </p>
        </li>
        <li>
          <h4>Universalism</h4>
          <p>
            An ethical theory that states that there is a system of norms and
            values that is universally applicable to everyone, independent of
            time, place, or culture.
          </p>
        </li>
        <li>
          <h4>Absolutism</h4>
          <p>
            A rigid form of universalism in which no exceptions to rules are
            possible. <br />
            Universalism allow for the possibility that not all norms and values
            are universal. <br />
            Absolutism does not make any exceptions: a rule is a rule.
          </p>
        </li>
        <li>
          <h4>Deontological ethics or deontology or duty ethics</h4>
          <p>
            We evaluate the action from the perspective of the action itself.
            <br />
            The point of departure is norms. It is your moral obligation to
            ensure that your actions agree with an applicable norm (rule or
            principle). (e.g. <a href="kant">Kant’s theory</a>)
          </p>
        </li>
        <li>
          <h4>Virtue ethics</h4>
          <p>
            We look at the actor and his/her characteristics to pass moral
            judgment on an action. <br />
            The moral point of departure is virtues, which allow people to
            realize a good life. (e.g.
            <a href="aristotele">Aristotle’s virtues doctrine</a>)
          </p>
        </li>
        <li>
          <h4>Consequentialism</h4>
          <p>
            We only consider the consequences. <br />
            The moral point of departure is values. (e.g.
            <a href="utilitarianism">Utilitarianism</a>)
          </p>
        </li>
        <li>
          <h4>Utilitarianism (Jeremy Bentham)</h4>
          <p>
            A type of consequentialism based on the utility principle. In
            utilitarianism, actions are judged by the amount of pleasure and
            pain they bring about (see <a href="#hedonism">hedonism</a>). The
            action that brings the greatest happiness for the greatest number
            should be chosen. <br />
            What is the purpose for which the action is a means? This purpose
            has to be something that has intrinsic value.
          </p>
          <h4>Issues</h4>
          <p>
            1) Happiness cannot be measured objectively. <br />
            2) Utilitarianism can lead to exploitation.<br />
            3) The consequences cannot be foreseen objectively and often are
            unpredictable, unknown, or uncertain. <br />
            4) Utilitarianism can lead to an unjust division of costs and
            benefits: no
            <a href="#distributive_justice">distributive justice</a>. Even if
            Hare's "decreasing
            <a href="#marginal_utility ">marginal utility</a>" principle solves
            this.<br />
            5) Utilitarianism ignores the personal relationships between
            people.<br />
            6) Certain actions are morally acceptable even though they do not
            create pleasure and some actions that maximize pleasure are morally
            unacceptable. <br />
          </p>
        </li>
        <li>
          <h4 id="hedonism">Hedonism</h4>
          <p>
            The idea that pleasure is the only thing that is good in itself and
            to which all other things are instrumental.
          </p>
        </li>
        <li>
          <h4>Utility principle</h4>
          <p>
            The principle that one should choose those actions that result in
            the greatest happiness for the greatest number.
          </p>
        </li>
        <li>
          <h4>Moral balance sheet</h4>
          <p>
            A balance sheet in which the costs and benefits (pleasures and
            pains) for each possible action are weighed against each other.
            Bentham proposed the drawing up of such balance sheets to determine
            the utility of actions. Cost-benefit analysis is a more modern
            variety of such balance sheets.
          </p>
        </li>
        <li>
          <h4>John Stuart Mill improvments to utilitarianism</h4>
          <p>
            According to Mill, qualities must be taken into account when
            applying the utilitarian calculus: forms of pleasure can be
            qualitatively compared, in which it is possible that a
            quantitatively smaller pleasure is preferred over a quantitatively
            larger one because the former pleasure is by nature more valuable
            than the latter.
          </p>
        </li>
        <li>
          <h4>Freedom principle</h4>
          <p>
            The moral principle that everyone is free to strive for his/her own
            pleasure, as long as they do not deny or hinder the pleasure of
            others.
          </p>
        </li>
        <li>
          <h4>No harm principle</h4>
          <p>
            The principle that one is free to do what one wishes, as long as no
            harm is done to others. Also known as the freedom principle.
          </p>
        </li>
        <li>
          <h4 id="marginal_utility ">Marginal utility</h4>
          <p>
            The additional utility that is generated by an increase in a good or
            service (income for example).
          </p>
        </li>
        <li>
          <h4 id="distributive_justice">Distributive justice</h4>
          <p>
            Distributive justice refers to the value of having a just
            distribution of certain important goods, like income, happiness, and
            career.
          </p>
        </li>
        <li>
          <h4>Act utilitarianism</h4>
          <p>
            The traditional approach to utilitarianism in which the rightness of
            actions is judged by the (expected) consequences of those actions.
          </p>
        </li>
        <li>
          <h4>Rule utilitarianism</h4>
          <p>
            A variant of utilitarianism that judges actions by judging the
            consequences of the rules on which these actions are based. These
            rules, rather than the actions themselves, should maximize utility.
          </p>
        </li>
        <li>
          <h4 id="kant">Kantian Theory</h4>
          <p>
            A core notion in Kantian ethics is autonomy. In Kant’s opinion man
            himself should be able to determine what is morally correct through
            reasoning. This should be possible independent of external norms,
            such as religious norms. The idea behind this is that we should
            place a moral norm upon ourselves and should obey it: it is our
            duty. We should obey this norm out of a sense of duty. According to
            Kant all moral laws can be derived from the categorical imperative.
          </p>
          <h5>Issues</h5>
          <p>
            1) In Kant’s theory there is no such thing as bending a rule. Kant
            does not allow for any exceptions in his theory. (e.g. lying is
            always bad, even to save someone's life), partially solved by
            <a href="#william_david_ross">William David Ross</a>
            2) After William David Ross correction, it still remains unclear how
            we should weigh the norms.
          </p>
        </li>
        <li>
          <h4>Good will</h4>
          <p>
            A central notion in Kantian ethics. According to Kant, we can speak
            of good will if our actions are led by the categorical imperative.
            Kant believes that the good will is the only thing that is
            unconditionally good.
          </p>
        </li>
        <li>
          <h4>Hypothetical norm</h4>
          <p>
            A condition norm, that is, a norm which only applies under certain
            circumstances, usually of the form “If you want X do Y.”
          </p>
        </li>
        <li>
          <h4>Categorical imperative</h4>
          <p>
            A universal principle of the form “Do A” which is the foundation of
            all moral judgments in Kant’s view.
          </p>
        </li>
        <li>
          <h4 id="universality_principle">Universality principle</h4>
          <p>
            First formulation of the categorical imperative: Act only on that
            maxim which you can at the same time will that it should become a
            universal law.
          </p>
        </li>
        <li>
          <h4>Equality postulate</h4>
          <p>
            The prescription to treat persons as equals, that is, with equal
            concern and respect.
          </p>
        </li>
        <li>
          <h4 id="reciprocity_principle">Reciprocity principle</h4>
          <p>
            Second formulation of the categorical imperative: Act as to treat
            humanity, whether in your own person or in that of any other, in
            every case as an end, never as means only.
          </p>
        </li>
        <li>
          <h4 id="william_david_ross">William David Ross improvments</h4>
          <p>
            Ross states that good is often situated on two levels: what seems to
            be good at first and that which is good once we take everything into
            consideration.
          </p>
        </li>
        <li>
          <h4>Prima facie norms</h4>
          <p>
            Prima facie norms are the applicable norms, unless they are
            overruled by other more important norms that become evident when we
            take everything into consideration (self-evident norms).
          </p>
        </li>
        <li>
          <h4>Kant vs William vs utilitarianism on child labor</h4>
          <p>
            Kant: “child labor is not permitted.” that is it. <br />
            William: The norm “children should not be forced into slavery or
            prostitution” would be the self-evident norm instead of “child labor
            is not permitted.”. <br />
            Utilitarianism: Children have less pain working rather than being
            enslaved.
          </p>
        </li>
        <li>
          <h4>Virtue Ethics</h4>
          <p>
            An ethical theory that focuses on the nature of the acting person.
            This theory indicates which good or desirable characteristics people
            should have or develop to be moral. <br />
            Aristotle argues that the good is sometimes ambiguous. However,
            people are not powerless in finding the middle course (
            <a href="#practical_wisdom">practical wisdom</a>).
          </p>

          <h5>Issues</h5>
          <p>
            Virtue ethics does not give concrete clues about how to act while
            solving a case, in contrast with utilitarianism and Kantian ethics.
            Even if having the right virtues does facilitate responsible action.
          </p>
        </li>
        <li>
          <h4>The good life</h4>
          <p>
            The highest good or eudaimonia: a state of being in which one
            realizes one’s uniquely human potential. According to Aristotle, the
            good life is the final goal of human action.
          </p>
        </li>
        <li>
          <h4>Moral virtue</h4>
          <p>
            Middle course between two extremes of evil. (e.g. courage is
            balanced between cowardice and recklessness) <br />
            Moral virtues are not given to us at birth nor are they
            supernatural; they can be developed by deeds.
          </p>
        </li>
        <li>
          <h4>Practical wisdom</h4>
          <p>
            The intellectual virtue that enables one to make the right choice
            for action. It consists in the ability to choose the right mean
            between two vices.
          </p>
        </li>
        <li>
          <h4>Kant's counter example to Aristotle's virtue ethics</h4>
          <p>
            Can we simply declare a moral virtue to be good in itself without
            any reservation? Kant’s example for this is a cold psychopath whose
            virtues moderation of conscience and passion, self control and cool
            deliberation make him much more terrible than he would have been
            without those virtues.
          </p>
        </li>
        <li>
          <h4>Michael Pritchard'Virtues for Morally Responsible Engineers</h4>
          <p>
            1) expertise/professionalism; <br />
            2) clear and informative communication;<br />
            3) cooperation;<br />
            4) willingness to make compromises;<br />
            5) objectivity;<br />
            6) being open to criticism;<br />
            7) stamina;<br />
            8) creativity;<br />
            9) striving for quality;<br />
            10) having an eye for detail; and<br />
            11) being in the habit of reporting on your work carefully. <br />
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>
        van de Poel, I., Royakkers, L. (2011). Ethics, Technology, and
        Engineering: An Introduction, Wiley. CHAPTER 4:
      </h3>
      <ul>
        <li>
          <h4>Argumentation</h4>
          <p>
            Is an activity which can be directed towards defending an opinion
            (attempted justification). <br />
            Or attacking an opinion (attempted refutation).
          </p>
        </li>
        <li>
          <h4>Argumentation theory</h4>
          <p>
            An interdisciplinary study of analyzing and evaluating arguments.
          </p>
        </li>
        <li>
          <h4>Argument</h4>
          <p>
            A set of statements, of which one (the conclusion) is claimed to
            follow from the others (the premises).
          </p>
        </li>
        <li>
          <h4>Conclusion of an argument</h4>
          <p>
            The statement that is affirmed on the basis of the premises of the
            argument.
          </p>
        </li>
        <li>
          <h4>Premises</h4>
          <p>
            The statements, which are affirmed (or assumed) as providing support
            or reasons for accepting the conclusion.
          </p>
        </li>
        <li>
          <h4 id="valid_argument">Valid argument</h4>
          <p>
            An argument whose conclusion follows with necessity from its
            premises: if the premises are true, the conclusion must be true.
            <br />
            With invalid arguments, the premises do not entail its conclusion.
          </p>
          <h5>Example</h5>
          <p>
            P1 : If it rains, the streets become wet. <br />
            P2 : It rains <br />
            C: The streets become wet.
          </p>
        </li>
        <li>
          <h4 id="modus_ponens">Modus ponens</h4>
          <p>
            Form of a valid argument in which the conclusion “q” follows from
            the premises “p” and “if p then q.”
          </p>
          <h5>Example</h5>
          <p>
            P1 : If it rains, the streets become wet. <br />
            P2 : It rains <br />
            C: The streets become wet.
          </p>
        </li>
        <li>
          <h4>Fallacy of affirming the consequent</h4>
          <p>"if p then q" does not imply that "if q then p".</p>
          <h5>Example</h5>
          <p>
            P′ 1 : If it rains, I will stay at home <br />
            P′ 2 : I will stay at home <br />
            C′: It rains
          </p>
        </li>
        <li>
          <h4>Modus tollens</h4>
          <p>
            Form of a valid argument in which the conclusion “not-p” follows
            from the premises “if p then q” and “not-q.”
          </p>
          <h5>Example</h5>
          <p>
            P″1: If it rains, I will stay at home <br />
            P″2: I will not stay at home <br />
            C″: It is not raining. <br />
          </p>
        </li>
        <li>
          <h4>Fallacy of denying the anteced</h4>
          <p>"if p then q" does not imply that "if q then p".</p>
          <h5>Example</h5>
          <p>
            P″′ 1 : If it rains, I will stay at home <br />
            P″′ 2 : It is not raining. <br />
            C″′: “I will not stay at home
          </p>
        </li>
        <li>
          <h4>When is it possible to challenge a conclusion?</h4>
          <table>
            <tr>
              <th></th>
              <th>All premises are true</th>
              <th>Some premises are false</th>
            </tr>
            <tr>
              <td>The argument is valid</td>
              <td>Conclusion cannot be challenged</td>
              <td>Conclusion cannot be challenged</td>
            </tr>
            <tr>
              <td>The argument is invalid</td>
              <td>Conclusion can be challenged</td>
              <td>Conclusion can be challenged</td>
            </tr>
          </table>
        </li>
        <li>
          <h4>Deductive (monotonic) arguments</h4>
          <p>
            The conclusion is enclosed in the premises: the result, the
            conclusion, says no more and is not logically stronger than the
            totality of premises that the argument is based on. <br />
            <a href="#valid_argument">Valid arguments</a> are of a deductive
            nature.
          </p>
        </li>
        <li>
          <h4>Non deductive (non monotonic) arguments</h4>
          <p>
            The conclusion is logically stronger than the premises. <br />
            The truth of the premises does not guarantee the truth of the
            conclusion. <br />
            Due to the indirect nature of non deductive argumentation, there
            always is a small degree of uncertainty.
          </p>
        </li>
        <li>
          <h4>Plausibility principle</h4>
          <p>
            The principle that enumeration and supplementary argumentation in a
            non-deductive argumentation can make the conclusion plausible
            (acceptable).
          </p>
        </li>
        <li>
          <h4>Inductive argumentation</h4>
          <p>
            A type of non-deductive argumentation. Argumentation from the
            particular to the general.
          </p>
        </li>
        <li>
          <h4>Critical questions</h4>
          <p>
            Questions belonging to a certain type of non deductive argumentation
            to check the degree of plausibility of a conclusion.
          </p>
        </li>
        <li>
          <h4>Sound argumentation</h4>
          <p>
            An argumentation for which the corresponding critical questions can
            be answered positively and which therefore makes the conclusion
            plausible if the premises are true.
          </p>
        </li>
        <li>
          <h4>Rash generalization</h4>
          <p>
            A fallacy related to inductive argumentation which occurs if the
            generalization is not sound, because too few observations were used
            as a basis for the general statement for example
          </p>
        </li>
        <li>
          <h4>Argumentation by analogy</h4>
          <p>
            If something is/was the case in an example case, then it also holds
            true for a comparable situation.
          </p>
          <h5>Formal description</h5>
          <p>
            Situation q is comparable with situation p (the analogy premise).
            <br />
            If situation p occurs then r applies.<br />
            So, if situation q occurs then r applies
          </p>
          <h5>Critical questions:</h5>
          <p>
            1) Are the two situations comparable? <br />
            2) Is what is asserted about the example situation true?
          </p>
        </li>
        <li>
          <h4>Arguments in a utilitarian plea</h4>
          <p>
            An action is morally acceptable if and only if that action can be
            reasonably expected to produce the greatest happiness for the
            greatest number of people. <br />
            In a utilitarian plea, the means-end argumentation is at the
            forefront. From a given end the means are derived to realize that
            end.
          </p>
        </li>
        <li>
          <h4 id="means_end">Means-end argumentation</h4>
          <p>If you wish to achieve end x, then you must carry out action y.</p>
          <h5>Formal description</h5>
          <p>
            x (the end) <br />
            Carrying out action y (the means) realizes the end x (the means-end
            premise) <br />
            So: do y
          </p>
          <h5>Critical questions</h5>
          <p>
            1) Does action y indeed realize end x? <br />
            2) Can action y be carried out? <br />
            3) Does execution of action y lead to unacceptable side effects?
            <br />
            4) Are there no other (better) actions to achieve x? <br />
            5) Is the end acceptable?
          </p>
        </li>
        <li>
          <h4>How to show that: "Does action y indeed realize end x?"</h4>
          <p>
            Two matters are of importance to answer this question.<br />
            1) We must demonstrate that the action leads to the expected
            consequence. (using
            <a href="#causality_argumentation ">causality argumentation </a> )
            <br />
            2) The consequence is the best one, that is, “the greatest happiness
            for the greatest number.” (using
            <a href="#comparative_assessment "> comparative assessment</a> )
            <a></a>
          </p>
        </li>
        <li>
          <h4 id="causality_argumentation">Causality Argumentation</h4>
          <p>
            In this argumentation, use is made of the fact that a certain
            expected consequence can be derived from a certain situation or
            action. <br />
            It is similar to the <a href="#modus_ponens">modus ponens</a>, but
            we can never be certain that q is the expected consequence of action
            p.
          </p>
          <h5>Formal description</h5>
          <p>
            p (action or situation) <br />
            So: q (the expected consequence)
          </p>
          <h5>Critical questions:</h5>
          <p>
            1) Will the given situation or action indeed lead to the expected
            consequence? <br />
            2) Have no issues been forgotten, for example, with respect to the
            expected consequence? <br />
            3) How do you determine the expected consequence and can it be
            justified?
          </p>
        </li>
        <li>
          <h4 id="comparative_assessment">Comparative assessment</h4>
          <p>
            Used to demonstrate that the consequence is indeed the best one.
            <br />
            This judgment can be made if the expected consequences of all
            possible actions have been determined, so that they can be compared.
            Using a kind of cost-benefit analysis, the best possible consequence
            is selected.
          </p>
        </li>
        <li>
          <h4>Frequently occurring fallacies in causality argumentations</h4>
          <p>
            1) Post hoc ergo propter hoc: a causal relationship is derived
            simply from the fact that two events occur after each other. (e.g.
            “I saw a black cat yesterday and then I saw a car accident. So black
            cats are bad luck.”) <br />
            2) Slippery slope: wild argumentation is involved: far-reaching
            consequences are derived from a small cause. (e.g. “You should not
            gamble. If you start you will not be able to stop. Soon you will
            lose all your money and end up in the gutter.”)
          </p>
        </li>
        <li>
          <h4>Argumentation in Kantian reasoning</h4>
          <p>
            1) An action is morally acceptable if and only if the action meets
            the first categorical imperative(
            <a href="#universality_principle">universality principle</a>).
            <br />
            2) An action is morally acceptable if and only if the action meets
            the second categorical imperative (
            <a href="#reciprocity_principle">reciprocity principle</a>).
          </p>
        </li>
        <li>
          <h4>
            Morally right actions according to the first categorical imperative
          </h4>
          <p>
            If we wish to defend that an action h is morally right, then we
            first take the negation of that action (not-h) and thus “not doing
            that action.” Next, we show that the action not-h is morally
            unacceptable by showing that the maxim of that action (i.e., the
            principle that the action is either permitted or forbidden for you)
            leads to a contradiction as soon as you make a general law of it
            (cf. Korsgaard, 1996). Thus we can say that h is morally acceptable.
            Basically
            <a href="#proof_from_the_absurd">proof from the absurd</a>.
          </p>
        </li>
        <li>
          <h4 id="proof_from_the_absurd">Proof from the absurd</h4>
          <p>
            The proposition is proven by showing that the negation of the
            proposition leads to a contradiction (an inconsistent set of
            statements).
          </p>
          <h5>Formal description</h5>
          <p>
            1) Assuming A (logically) leads to an inconsistent set of statements
            <br />
            2) So: not-A
          </p>
          <h5>Critical questions</h5>
          <p>
            1) Does assuming A indeed lead to an inconsistent set of statements?
            <br />
            2) Is not-A (or not-p) indeed the negation of A (or p)? In other
            words, can A be concluded from “not not-A?”
          </p>
        </li>
        <li>
          <h4>
            Morally right actions according to the second categorical imperative
          </h4>
          <p>
            If it is my aim to obtain money from someone without giving it back,
            I can realize this by means of a false promise or by misleading
            someone (the means). Next, we must ask the question whether the
            person I made the promise to would agree to the means and the end it
            is supposed to meet. If the answer is positive then the action is
            morally acceptable, but if the answer is negative (as in this case)
            the action is morally unacceptable. Basically
            <a href="#means_end">ends-means</a> form of argumentation.
          </p>
        </li>
        <li>
          <h4>Argumentation in virtue-ethical reasoning</h4>
          <p>
            An action is morally acceptable if and only if that action is what a
            virtuous agent would do in the circumstances. (Hursthouse, 1991)
            <br />
            The question is, of course, how we define a virtuous person. <br />
            Using
            <a href="#characteristic_judgment_argumentation "
              >Characteristic-judgment argumentation
            </a>
          </p>
        </li>
        <li>
          <h4 id="characteristic_judgment_argumentation">
            Characteristic-Judgment Argumentation
          </h4>
          <p>
            If someone or something X displays certain characteristics s1 ,s2
            ,…, sn , then judgment A is justified for that person or thing.
          </p>
          <h5>Formal description</h5>
          <p>
            1) has the characteristics s1 ,s2 ,…,sn <br />
            2) characteristics s1 ,s2 ,…,sn are typical of A (the
            characteristic-judgment premise) <br />
            3) So: A applies to X
          </p>
          <h5>Critical questions</h5>
          <p>
            1) Do the characteristics mentioned justify judgment A?<br />
            2) Are the characteristics mentioned all typical of A?<br />
            3) Are there any other characteristics necessary for A?<br />
            4) Does X possess characteristics that justify the judgment not
            A?<br />
            5) Does X possess the characteristics mentioned?<br />
          </p>
        </li>
        <li>
          <h4>Formal fallacies</h4>
          <p>
            Formal fallacies are determined solely by the form (hence the name)
            or structure of an argument. Any invalid argument is a formal
            fallacy.
          </p>
        </li>
        <li>
          <h4>Informal fallacies</h4>
          <p>
            Informal fallacies are based on considerations of the context and
            content of the arguments. Informal fallacies which are often be used
            in ethical discussions are:
            <a href="#attack_on_the_person">attack on the person</a>;
            <a href="#confusion_of_law_and_ethics"
              >confusion of law and ethics</a
            >; <a href="#straw_person">straw person</a>;
            <a href="#wishful_thinking">wishful thinking</a>;
            <a href="#naturalistic_fallacy">naturalistic fallacy</a>;
            <a href="#privacy_fallacy">privacy fallacy</a>; and
            <a href="#ambiguity">ambiguity</a>.
          </p>
        </li>
        <li>
          <h4 id="attack_on_the_person">Attack on the person (Ad Hominem)</h4>
          <p>
            This fallacy is committed when an attempt is made to discredit an
            argument by bringing into question in some negative way the
            presenter of the argument instead of attacking the argument itself.
          </p>
        </li>
        <li>
          <h4 id="confusion_of_law_and_ethics">Confusion of law and ethics</h4>
          <p>
            Confusion of law and ethics can simply be put as “If it isn’t
            illegal, it is ethical.” Ethics is, however, more encompassing than
            law. (e.g. Having an extramarital affair is not forbidden by law,
            but that is not to say that this is morally acceptable)
          </p>
        </li>
        <li>
          <h4 id="straw_person">Straw person</h4>
          <p>
            The straw person fallacy is committed when an attempt is made to
            miss-state a person’s actual position and conclude that the original
            argument is a bad argument.
          </p>
          <h5>Example</h5>
          <p>
            Suppose that someone is critical of the use of nuclear energy, and
            his opponent react as follows: “Personally, I think the energy power
            supply is of the highest importance.” With this the opponent
            suggests that the first one does not adhere to good energy power
            supply, but this cannot be derived from his criticism on nuclear
            energy.
          </p>
        </li>
        <li>
          <h4 id="wishful_thinking">Wishful thinking</h4>
          <p>
            Wishful thinking (or fallacy of desire) occurs when a person
            interprets facts, reports, events, perceptions, and so on, according
            to what he/she would like to be the case rather than according to
            the actual or rational acceptable evidence.
          </p>
        </li>
        <li>
          <h4 id="naturalistic_fallacy">Naturalistic fallacy</h4>
          <p>
            The naturalistic fallacy takes the form of deducing normative
            statements based only on descriptive statements. We can simply put
            it as: deriving ought from is. The (false) reason behind this
            fallacy is that we must always accept things as they are.
          </p>
          <h5>Example</h5>
          <p>
            “Stealing bikes is morally acceptable in the Netherlands, because
            more bikes are stolen than bought in the Netherlands.”
          </p>
        </li>
        <li>
          <h4 id="privacy_fallacy">Privacy fallacy</h4>
          <p>
            If you have done nothing wrong, you have nothing to worry about.
            That argument does not hold for several reasons: <br />
            1) Erroneous information can dramatically affect your life. <br />
            2) A person can risk discrimination, if that person’s information is
            publicly known.<br />
          </p>
        </li>
        <li>
          <h4 id="ambiguity">Ambiguity</h4>
          <p>
            Fallacies of ambiguity play with the meaning of words or phrases and
            therefore often are humorous. <br />
            One must be careful if words or phrases can have different
            interpretations, because it can make a significant difference to the
            meaning of what is said and can therefore generate fallacious
            inferences.
          </p>
        </li>
        <li>
          <h4>Fallacies of risk</h4>
          <p>
            Fallacies specific to public debates on the acceptability of
            technological risks. <br />
            NB: from now on X and Y stand for an activity, product or
            technology.
          </p>
        </li>
        <li>
          <h4>The sheer size fallacy</h4>
          <p>
            1) X is accepted <br />
            2) Y is a smaller risk than X <br />
            3) So, Y should be accepted <br />
            This fallacy is based on a false analogy. The analogy can only be
            made in a right way, if X and Y are alternatives in the same
            decision.
          </p>
        </li>
        <li>
          <h4>The fallacy of naturalness</h4>
          <p>
            1) X is unnatural
            <br />
            2) So, X should not be accepted <br />
            This fallacy corresponds with the naturalistic fallacy. A normative
            statement is derived from only a descriptive statement. The idea
            behind this fallacy is that whatever is unnatural is wrong. The
            problem with this is that is not clear what is meant by “natural.”
          </p>
        </li>
        <li>
          <h4>The ostrich’s fallacy</h4>
          <p>
            1) X does not give rise to any detectable risk or there is no
            scientific proof that X is dangerous <br />
            2) So, X does not give rise to any unacceptable risk
            <br />
            The gist of the fallacy is that as long as a risk does not reveal,
            the risk does not exist.
          </p>
        </li>
        <li>
          <h4>The delay fallacy</h4>
          <p>
            1) If we wait we will know more about X <br />
            2) So, no decision about X should be made now <br />
            But the problem may get worse, or change.
          </p>
        </li>
        <li>
          <h4>The technocratic fallacy</h4>
          <p>
            1) It is an engineering issue how dangerous X is <br />
            2) So, engineers should decide whether or not X is acceptable <br />
            Indeed, engineers have competence in determining the nature and the
            magnitude of technological risks, but this competence is not the
            same as competence in deciding whether or not a technological risk
            is morally acceptable. (see <a href="#technocracy">technocracy</a>)
          </p>
        </li>
        <li>
          <h4 id="privacy_fallacy">The fallacy of pricing</h4>
          <p>
            1) We have to weight the risks of X against its benefits<br />
            2) So, we must put a price on the risks of X<br />
            There are many things we cannot easily value in terms of money,
            including those that involve the loss of human lives. Furthermore,
            there is no sensible price that can be meaningfully assigned to
            technological risks, since it also depends on the circumstances.
          </p>
        </li>
        <li>
          <h4 id="privacy_fallacy">Privacy fallacy</h4>
          <p></p>
        </li>
      </ul>
    </section>
    <section>
      <h3>
        Winner, L. (1980). “Do artifacts have politics?”. Daedalus, 109,
        121-136:
      </h3>
      <ul>
        <li>
          <h4 id="lewis_mumford">Lewis Mumford' two types of technologies</h4>
          <p>
            1) Authoritarian: system-centered, immensely powerful, but
            inherently unstable. <br />
            2) Democratic: a-centered, relatively weak, but resourceful and
            durable.
          </p>
        </li>
        <li>
          <h4>Social determination of technology</h4>
          <p>
            What matters is not technology itself, but the social or economic
            system in which it is embedded. This goes both for technological
            development and usage.
          </p>
          <h5>Upsides</h5>
          <p>
            Those who have not recognized the ways in which technologies are
            shaped by social and economic forces have not gotten very far.
          </p>
          <h5>Downsides</h5>
          <p>
            Taken literally, it suggests that technical things do not matter at
            all. <br />
            This makes it easy for social scientists: nothing special,
            preexisting models are not influenced and still valid.
          </p>
        </li>
        <li>
          <h4>Theory of technological politics</h4>
          <p>
            It is true that technologies are shaped by social and economic
            forces, but also the vice versa: society change and advance based on
            the technological advancments, humans adapt to technology.
          </p>
        </li>
        <li>
          <h4>Ways in which artifacts can contain political properties.</h4>
          <p>
            <a href="#technical_arrangments"
              >Technical Arrangments as Forms of Order</a
            >; <br />
            <a href="#inherently_political">Inherently Political Technologies</a
            >; <br />
          </p>
        </li>
        <li>
          <h4>Technical Arrangments as Forms of Order</h4>
          <p>
            Usualy technologies are seen as neutral tools that can be used well
            or poorly, for good, evil, or something in between. <br />
            But what if a given device has been designed and built in such a way
            that it produces a set of consequences logically and temporarly
            prior to any of its professed uses? <br />
            Some features in the design or arrangment of a device or system
            could provide a convenient means for estabilishing patterns of power
            and authority in a given setting.
          </p>
          <h5>Example</h5>
          <p>
            Robert Moses designned purposedly low bridges over parkways, so that
            buses full of black and low income people would not be able to use
            thoose parkways.
          </p>
        </li>
        <li>
          <h4>
            Choices affecting distribution of power, authority and priviledge.
          </h4>
          <p>
            1) Yes or no: are we going to develop the new technology or not?<br />
            2) How: specific features in the design or arrangment of a technical
            system after the decision to go ahead with it has already been made.
            Making theese choices is complex: not everyone is aware in the same
            way about how a technology will influence society. <br />
            The highest level of choice is at the start, because once a standard
            is taken choices are more and more fixed. But at he start usually
            little is known about the consequences.
          </p>
        </li>
        <li>
          <h4>Technology impacts society</h4>
          <p>
            The issues that divide or unite people in society are settled not
            only in the institutions and practices of politics proper, but also,
            in tangible arrangments of steel and concrete, wires and
            transistors, nuts and bolts.
          </p>
        </li>
        <li>
          <h4 id="inherently_political">Inherently Political Technologies</h4>
          <p>
            Some technologies are by their very nature political in a specific
            way: centralized or decentralized, egalitarian or inegalitarian,
            repressive or liberating. As
            <a href="#lewis_mumford">Lewis Mumford</a> said.<br />
            Usually, especyally in the case of large scale technologu, they are
            strongly linked to particular institutionalized patterns of power
            and authority.
          </p>
        </li>
        <li>
          <h4>Engel's technological authority</h4>
          <p>
            Technology is inherently hierarchical, operating a machine
            necesitates order and coordination, granted by a hierarchical scheme
            where, at the top, there is the "imperious authority" of the
            machine. (this is a very extremist view)
          </p>
          <h5>Example</h5>
          <p>
            If you accept nuclear power plants, you also accept a
            techno-scientific-industrial-military elite.
          </p>
        </li>
        <li>
          <h4>Marx's position in "Capital"</h4>
          <p>
            Increasing mechanization will render obsolete the hierarchical
            division of labor and the relationship of subordination that, in his
            view, were necessary during the early stages of modern
            manufacturing.
          </p>
        </li>
        <li>
          <h4>A weaker way of stating the case political technologies</h4>
          <p>
            A given kind of technology is strongly compatible with, but does not
            strictly require, social and political relationships of a particular
            stripe.
          </p>
          <h5>Example</h5>
          <p>
            Solar energy is decentralized both in a technical and political
            sense, but there is nothing about it that strictly requires
            democracy.
          </p>
        </li>
        <li>
          <h4>Chandler's opinion on large scale systems</h4>
          <p>
            The properties of large scale systems require centralized,
            hierarchical managerial control. (e.g. railroads, atomic bomb)
          </p>
        </li>
        <li>
          <h4>Plato's ship</h4>
          <p>
            A ship at sea needs steering by a decisive hand. This can be
            accomplished only by a single capitan and an obedient crew.
          </p>
        </li>
        <li>
          <h4>Politics of technology influences the politics of community</h4>
          <p>
            While in theory the two things should be separated, it has been
            noted that patterns of autority that work effectively in the
            corporation become for buisnessmen "the desirable model against
            which to compare political and economic relationships in the rest of
            society".
          </p>
        </li>
        <li>
          <h4>The cost of centralization</h4>
          <p>
            High risk centralizated technologies (e.g. nuclear plants based on
            plutonium) are better handled by an authoritarian centralized entity
            that can keep security (e.g. avoid stealing of plutonium), but at
            what cost for the worker's and citizen's freedom?
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>
        Rachels, J. (1975). “Why Privacy is Important”, Philosophy & Public
        Affairs, Vol. 4, No. 4, pp. 323-333:
      </h3>
      <p>James Rachels against Thomas Scanlon simplification</p>
      <ul>
        <li>
          <h4>
            The first element of a theory of privacy according to Thomas Scanlon
          </h4>
          <p>
            A characterization of the special interest we have in being able to
            be free from certain kinds of intrusions. <br />
            Right place to begin.
          </p>
        </li>
        <li>
          <h4>Examples of why privacy is important to us</h4>
          <p>
            a) Privacy is sometimes necessary to protect people's interests in
            competitive situations. <br />
            b) In other cases someone may want to keep some aspect of his life
            or behaviour private simply because it would be embarrassing for
            other people to know about it. <br />
            c) There are several reasons why medical records should be kept
            private, having to do with the consequences to individuals of facts
            about them becoming public knowledge. <br />
            d) When people apply for credit (or for large amounts of insurance
            or for jobs of certain types) they are often investigated, and the
            result is a fat file of information about them. <br />
            All sorts of other information goes into such files, for example,
            information about the applicant's sex-life, his political views, and
            so forth. Clearly it is unfair for one's application for credit to
            be influenced by such irrelevant matters. <br />
          </p>
        </li>
        <li>
          <h4>Why privacy is more than that</h4>
          <p>
            It is "nobody else's buisness".<br />
            We have a "sense of privacy' which is violated in such affairs, and
            this sense of privacy cannot adequately be explained merely in terms
            of our fear of being embarrassed or disadvantaged in one of these
            obvious ways.
          </p>
          <h5>Example</h5>
          <p>
            A woman may rightly be upset if her creditrating is adversely
            affected by a report about her sexual behaviour because the use of
            such information is unfair. <br />
            But more importantly she may also object to the report simply
            because she feels-as most of us do-that her sex-life is nobody
            else's business.
          </p>
        </li>
        <li>
          <h4>Privacy and relationships</h4>
          <p>
            Privacy is necessary if we are to maintain the variety of social
            relationships with other people that we want to have and that is why
            it is important to us. <br />
            There are different patterns of behaviour associated with different
            relationships.
          </p>
        </li>
        <li>
          <h4>
            Changing behaviour according to the relationship means dishonesty
            (anthitesis)
          </h4>
          <p>
            "Real people aren't any different when they're alone. No masks. What
            you see of them is authentic." <br />
            According to this way of looking at things, the fact that we observe
            different standards of conduct with different people is merely a
            sign of dishonesty
          </p>
        </li>
        <li>
          <h4>
            Different relationships are characterized by different behaviour
          </h4>
          <p>
            It is not merely accidental that we vary our behaviour with
            different, people according to the different social relationships
            that we have with them. <br />
            Rather, the different patterns of behaviour are (partly) what define
            the different relationships; they are an important part of what
            makes the different relationships what they are <br />
            However one conceives one's relations with other people, there is
            inseparable from that conception an idea of how it is appropriate to
            behave with and around them.
          </p>
        </li>
        <li>
          <h4 id="different_behaviour">
            Different relationships are characterized different information
            disclosure
          </h4>
          <p>
            However one conceives one's relations with other people, there is
            inseparable from that conception an idea of what information about
            oneself it is appropriate for them to have.
          </p>
        </li>
        <li>
          <h4>Privacy is crucial to relationships</h4>
          <p>
            Our ability to control who has access to us, and who knows what
            about us, allows us to maintain the variety of relationships with
            other people that we want to have, it is, one of the most important
            reasons why we value privacy.
          </p>
          <h5>Example</h5>
          <p>
            First, consider what happens when two close friends are joined by a
            casual acquaintance. The character of the group changes; and one of
            the changes is that conversation about intimate matters is now out
            of order. Then suppose these friends could never be alone; suppose
            there were always third parties (let us say casual acquaintances or
            strangers) intruding. Then they could do either of two things. They
            could carry on as close friends do, sharing confidences, freely
            expressing their feelings about things, and so on. But this would
            mean violating their sense of how it is
            <a href="#different_behaviour">appropriate to behave</a> around
            casual acquaintances or strangers.
          </p>
        </li>
        <li>
          <h4>Thomson's simplifying hypothesis</h4>
          <p>
            The right to privacy is itself a cluster of rights, and that it is
            not a distinct cluster of rights but itself intersects with the
            cluster of rights which the right over the person consists of, and
            also with the cluster of rights which owning property consists of."
            This hypothesis is "simplifying" because it eliminates the right to
            privacy as anything distinctive.
          </p>
          <h5>Counter example by James (author)</h5>
          <p>
            Consider the right not to have various parts of one's body looked
            at.
            <br />
            Insofar as this is a matter of privacy, it is not simply analogous
            to property rights; for the kind of interest we have in controlling
            who looks at what parts of our bodies is very different from the
            interest we have in our cars or fountain pens.<br />
            By dissociating the body from ideas of physical intimacy, and the
            complex of personal relationships of which such intimacies are a
            part, we can make this "right over the body" seem to be nothing more
            than an un-grand kind of property right; but that dissociation
            separates this right from the matters that make privacy important
          </p>
        </li>
        <li>
          <h4>Thomson's very personal gossip</h4>
          <p>
            Does it violate your right to privacy for others to indulge in "very
            personal gossip" about you? She thinks not.
          </p>
          <h5>Counter example by James (author)</h5>
          <p>
            You have shared your troubles with your closest friend, but this is
            not the sort of thing you want everyone to know. Not only would it
            be humiliating for everyone to know, it is none of their business.
            It is the sort of intimate fact about you that is not appropriate
            for strangers or casual acquaintances to know. But now the gossips
            have obtained the information (perhaps one of them innocently
            overheard your discussion with your friend; it was not his fault, so
            he did not violate your privacy in the hearing, but then you did not
            know he was within earshot) and now they are spreading it around to
            everyone who knows you and to some who do not. Are they violating
            your right to privacy? I think they are.
          </p>
          <h5>Conclusion</h5>
          <p>
            Since the right that is violated in this case is not also a property
            right, or a right over the person, the simplifying hypothesis fails.
          </p>
        </li>
        <li>
          <h4>The simplifying hypothesis is not enough</h4>
          <p>
            If the right to privacy has a different point than these other
            rights, we should not expect it always to overlap with them. <br />
            And even if it did always overlap, we could still regard the right
            to privacy as a distinctive sort of right in virtue of the special
            kind of interest it protects.
          </p>
        </li>
      </ul>
    </section>
    <section id="privacy_and_information_technology">
      <h3>
        van den Hoven, J., Blaauw, M., Pieters, W. and Warnier, M., "Privacy and
        Information Technology"
        <a href="https://plato.stanford.edu/entries/it-privacy/"
          >The Stanford Encyclopedia of Philosophy (Winter 2014 Edition)</a
        >, Edward N. Zalta (ed.):
      </h3>
      <p>
        The focus of this article is on exploring the relationship between
        information technology and privacy. <br />
        We will both illustrate the specific
        <a href="#internet_and_privacy"
          >threats that IT and innovations in IT
        </a>
        pose for privacy and indicate
        <a href="#it_solutions"
          >how IT itself might be able to overcome these privacy concerns</a
        >
        by being developed in ways that can be termed “privacy-sensitive”,
        “privacy enhancing” or “privacy respecting”.
        <br />
        We will also discuss the role of
        <a href="#emerging_technologies"> emerging technologies </a>
        in the debate, and account for the way in which moral debates are
        themselves affected by IT.
      </p>
      <ul>
        <li>
          <h4>Surveillance capitalism</h4>
          <p>
            Label used to describe the scope and purpose of the personal data
            centred business models of Big Tech
          </p>
        </li>
        <li>
          <h4>Constitutional (decisional) privacy</h4>
          <p>
            The freedom to make one’s own decisions without interference by
            others in regard to matters seen as intimate and personal.
          </p>
        </li>
        <li>
          <h4>Tort (informational) privacy</h4>
          <p>
            The interest of individuals in exercising control over access to
            information about themselves.
          </p>
          <h5>Normative definition</h5>
          <p>
            Informational privacy in a normative sense refers typically to a
            nonabsolute moral right of persons to have direct or indirect
            control over access to (1) information about oneself, (2) situations
            in which others could acquire information about oneself, and (3)
            technology that can be used to generate, process or disseminate
            information about oneself
          </p>
        </li>
        <li>
          <h4>Opinions on the inpact of new technology on privacy</h4>
          <p>
            1) We have zero privacy in the digital age and that there is no way
            we can protect it, so we should get used to the new world and get
            over it (Sprenger 1999). <br />
            2) Our privacy is more important than ever and that we can and we
            must attempt to protect it.
          </p>
        </li>
        <li>
          <h4>Privacy in reductionist accounts</h4>
          <p>
            Privacy claims are really about other values and other things that
            matter from a moral point of view.
          </p>
          <h5>Opposing view</h5>
          <p>
            Privacy is valuable in itself and its value and importance are not
            derived from other considerations.
          </p>
        </li>
        <li>
          <h4>Privacy in cluster accounts</h4>
          <p>
            There is a cluster of related moral claims underlying appeals to
            privacy, but maintains that there is no single essential core of
            privacy concerns.
          </p>
        </li>
        <li>
          <h4>Privacy in epistemic accounts</h4>
          <p>
            Having privacy means that others don’t know certain private
            propositions; lacking privacy means that others do know certain
            private propositions (Blaauw 2013).
          </p>
        </li>
        <li>
          <h4>EU vs US on privacy</h4>
          <p>
            EU conceptualizes issues of informational privacy in terms of ‘data
            protection’, US in terms of ‘privacy’ (Heersmink et al. 2011).
          </p>
        </li>
        <li>
          <h4>Personal data</h4>
          <p>
            Personal data is defined in the law as data that can be linked with
            a natural person. There are two ways in which this link can be made:
            a referential mode and a non-referential mode.
          </p>
        </li>
        <li>
          <h4>Referential mode</h4>
          <p>
            The type of use that is made on the basis of a (possible)
            acquaintance relationship of the speaker with the object of his
            knowledge.
          </p>
          <h5>Example</h5>
          <p>
            “The murderer of Kennedy must be insane”, uttered while pointing to
            him in court is an example of a referentially used description.
          </p>
        </li>
        <li>
          <h4>Non referential mode</h4>
          <p>
            The user of the description is not – and may never be – acquainted
            with the person he is talking about or intends to refer to.
          </p>
          <h5>Example</h5>
          <p>“the murderer of Kennedy must be insane, whoever he is”.</p>
        </li>
        <li>
          <h4>Moral reasons for the protection of personal data</h4>
          <p>
            <a href="#prevention_of_harm">Prevention of harm</a>,
            <a href="#informational_inequality">informational inequality</a>,
            <a href="#informational_injustice"
              >informational injustice and discrimination</a
            >,
            <a href="#violation_of_personal"
              >Violation of personal choice and inherent human worth.</a
            >,
          </p>
        </li>
        <li>
          <h4 id="prevention_of_harm">Prevention of harm</h4>
          <p>
            Unrestricted access by others can be used to harm the data subject
            in a variety of ways.
          </p>
        </li>
        <li>
          <h4 id="informational_inequality">Informational inequality</h4>
          <p>
            Individuals are usually not in a good position to negotiate
            contracts about the use of their data and do not have the means to
            check whether partners live up to the terms of the contract.
          </p>
        </li>
        <li>
          <h4 id="informational_injustice">
            Informational injustice and discrimination
          </h4>
          <p>
            Personal information provided in one sphere or context may change
            its meaning when used in another sphere or context and may lead to
            discrimination and disadvantages for the individual.
          </p>
        </li>
        <li>
          <h4 id="violation_of_personal">
            Violation of personal choice and inherent human worth
          </h4>
          <p>
            1) Mass surveillance leads to a situation where routinely,
            systematically, and continuously individuals make choices and
            decisions because they know others are watching them. <br />
            2) Being able to figure people out on the basis of their big data
            constitutes an epistemic and moral immodesty (Bruynseels & Van den
            Hoven 2015), which fails to respect the fact that human beings are
            subjects with private mental states that have a certain quality that
            is inaccessible from an external perspective.
          </p>
        </li>
        <li>
          <h4>Law, regulation, and indirect control over access</h4>
          <p>
            Data protection laws are in force in almost all countries. The basic
            moral principle underlying these laws is the requirement of informed
            consent. <br />
            But it is impossible to guarantee compliance of all types of data
            processing in all these areas and applications with these rules and
            laws in traditional ways. <br />
            Hence “privacy-enhancing technologies” (PETs) and identity
            management systems are expected to replace human oversight in many
            cases.
          </p>
        </li>
        <li>
          <h4 href="privacy_by_design">Privacy by design</h4>
          <p>
            The data ecosystems and socio-technical systems, supply chains,
            organisations, including incentive structures, business processes,
            and technical hardware and software, training of personnel, should
            all be designed in such a way that the likelihood of privacy
            violations is a low as possible. <br />
            “data protection needs to be viewed in proactive rather than
            reactive terms, making privacy by design preventive and not simply
            remedial” (Cavoukian 2010).
          </p>
        </li>
        <li id="internet_and_privacy">
          <h4>Internet and privacy</h4>
          <p>
            The Internet was not designed for the purpose of separating
            information flows (Michener 1999). Sites are allowed to store
            information on your computer, this is necessary for them to work.
            But this same information can be used, and is used, for tracking.
          </p>
        </li>
        <li>
          <h4>Cloud and privacy</h4>
          <p>
            In cloud computing, both data and programs are online (in the
            cloud), and it is not always clear what the user-generated and
            system-generated data are used for. Moreover, as data are located
            elsewhere in the world, it is not even always obvious which law is
            applicable, and which authorities can demand access to the data.
          </p>
        </li>
        <li>
          <h4>Social media and privacy</h4>
          <p>
            Users are tempted to exchange their personal data for the benefits
            of using services, and provide both this data and their attention as
            payment for the services. <br />
            In addition, users may not even be aware of what information they
            are tempted to provide.
          </p>
          <h5>Countermesures and limitations</h5>
          <p>
            One way of limiting the temptation of users to share is requiring
            default privacy settings to be strict.<br />
            Such restrictions limit the value and usability of the social
            network sites themselves, and may reduce positive effects of such
            services. A particular example of privacy-friendly defaults is the
            opt-in as opposed to the opt-out approach.
          </p>
        </li>
        <li>
          <h4>Big data and privacy</h4>
          <p>
            Big data may be used in profiling the user (Hildebrandt 2008),
            creating patterns of typical combinations of user properties, which
            can then be used to predict interests and behaviour. <br />
            Profiling could also be used by organizations or possible future
            governments that have discrimination of particular groups on their
            political agenda, in order to find their targets and deny them
            access to services, or worse.
          </p>
          <h5>Countermesures and limitations</h5>
          <p>
            According to EU data protection law, permission is needed for
            processing personal data, and they can only be processed for the
            purpose for which they were obtained. <br />
            Specific challenges, therefore, are: <br />
            1) How to obtain permission when the user does not explicitly engage
            in a transaction (as in case of surveillance). <br />
            2) How to prevent “function creep”, i.e. data being used for
            different purposes after they are collected (as may happen for
            example with DNA databases (Dahl & Sætnan 2009).
          </p>
        </li>
        <li>
          <h4>Mobile devices and privacy</h4>
          <p>
            These devices typically contain a range of data-generating sensors
            (e.g. cameras). It is supposed that the user knows when theese
            sensors are active or not. (e.g. a light warning when the camera is
            active) But malicious software might be avoiding this. <br />
            In general “reconfigurable technology” that handles personal data
            raises the question of user knowledge of the configuration.
          </p>
        </li>
        <li>
          <h4>IOT and privacy</h4>
          <p>
            EU and US passports have RFID chips with protected biometric data,
            but information like the user’s nationality may easily leak when
            attempting to read such devices. <br />
            Even dumb chips (chips that only contain a number) could be used to
            trace a person once it is known that he carries an item containing a
            chip. <br />
            This is just the tip of the iceberg: many devices contain sensors
            which may communicate with the company in order to gather data.
          </p>
        </li>
        <li>
          <h4>E-Government and privacy</h4>
          <p>
            In polling stations, the authorities see to it that the voter keeps
            the vote private, but such surveillance is not possible when voting
            by mail or online, and it cannot even be enforced by technological
            means, as someone can always watch while the voter votes. In this
            case, privacy is not only a right but also a duty, and information
            technology developments play an important role in the possibilities
            of the voter to fulfill this duty, as well as the possibilities of
            the authorities to verify this. In a broader sense, e-democracy
            initiatives may change the way privacy is viewed in the political
            process.
          </p>
        </li>
        <li>
          <h4>Surveillance and privacy</h4>
          <p>
            Information technology is used for all kinds of surveillance tasks.
            It can be used to augment and extend traditional surveillance
            systems, or to create a whole new kind of surveillance:
            "surveillance capitalism" where social media and other online
            systems are used to gather large amounts of data about individuals.
            <br />
            In addition to the private sector surveillance industry, governments
            form another traditional group that uses surveillance techniques at
            a large scale, either by intelligence services or law enforcement.
          </p>
        </li>
        <li id="it_solutions">
          <h4>IT solutions to privacy concerns</h4>
          <p>
            There are rules, guidelines or best practices that can be used for
            designing privacy-preserving systems. Such possibilities range from
            ethicallyinformed design methodologies to using encryption to
            protect personal information from unauthorized use.
          </p>
        </li>
        <li>
          <h4>Privacy engineering</h4>
          <p>
            Extends the
            <a href="#privacy_by_design">privacy by design</a> approach by
            aiming to provide a more practical, deployable set of methods by
            which to achieve system-wide privacy. <br />
            Systems that are designed with these rules and guidelines in mind
            should thus – in principle – be in compliance with EU privacy laws
            and respect the privacy of its users. <br />
          </p>
          <h5>Issues</h5>
          <p>
            1) But different people will interpret the principles
            differently.<br />
            2) During the implementation phase software bugs are introduced.
            <br />
            3) It is very hard to verify whether an implementation meets its
            design/specification.
          </p>
        </li>
        <li>
          <h4>Privacy enhancing technologies</h4>
          <p>
            In Tor, messages are encrypted and routed along numerous different
            computers. Similarly, in Freenet content is stored in encrypted form
            from all users of the system (but keys are not shared!). Both theese
            technologies plausible deniability and privacy. <br />
            Another option for providing anonymity is the anonymization of data
            through special software. But it is very hard to
            <a href="#anonymous_identifiers"
              >anonymize data in such a way that all links with an individual
              are removed</a
            >
            and the resulting anonymized data is still useful for research
            purposes.
          </p>
        </li>
        <li>
          <h4>Cryptography</h4>
          <p>
            No need to say obvious things (it is important and widely used,
            hashes, SSO, HTTPS, VPN...). Below we show 2 interesting
            techniques.<br />
            1) Homomorphic encryption allows a data processor to process
            encrypted data, i.e. users could send personal data in encrypted
            form and get back some useful results – for example, recommendations
            of movies that online friends like – in encrypted form. <br />
            2) Blockchain technology, although focused on data integrity and not
            inherently anonymous, enables many privacy-related applications.
          </p>
        </li>
        <li>
          <h4>Identity managment</h4>
          <p>
            It is quite common for ‘Single sign on’ frameworks (OpenID, Login
            with Google...) to be linked to the real world identity of
            individuals. <br />
            Requiring a direct link between online and ‘real world’ identities
            is problematic from a privacy perspective, because they allow
            profiling of users <br />
            Blockchain technology is used to make it possible for users to
            control a digital identity without the use of a traditional trusted
            third party (Baars 2016).
          </p>
        </li>
        <li>
          <h4 id="emerging_technologies">How will things evolve in future?</h4>
          <p>
            However, there are future and emerging technologies that may have an
            even more profound impact. Consider for example brain-computer
            interfaces. In case computers are connected directly to the brain,
            not only behavioural characteristics are subject to privacy
            considerations, but even one’s thoughts run the risk of becoming
            public, with decisions of others being based upon them. In addition,
            it could become possible to change one’s behaviour by means of such
            technology.
          </p>
        </li>
        <li>
          <h4>Privacy evolves together with technology</h4>
          <p>
            Technology does not only influence privacy by changing the
            accessibility of information, but also by changing the privacy norms
            themselves: something that might be arguably private and worth
            protecting today, might become universally accepted as shared.
          </p>
        </li>
        <li>
          <h4>Focus on data usage rather than acquisition</h4>
          <p>
            1) It may be more feasible to protect privacy by transparency – by
            requiring actors to justify decisions made about individuals, thus
            insisting that decisions are not based on illegitimate information.
            <br />
            2) It may well happen that citizens, in turn, start data collection
            on those who collect data about them. The
            <a href="#why_software_should_be_free">open source</a>
            movement may also contribute to transparency of data processing. In
            this context, transparency can be seen as a pro-ethical condition
            contributing to privacy.
          </p>
        </li>
        <li>
          <h4>Precautionary principle</h4>
          <p>
            The burden of proof for absence of irreversible effects of
            information technology on society, e.g. in terms of power relations
            and equality, would lie with those advocating the new technology.
          </p>
        </li>
      </ul>
    </section>
    <section id="big_data_end_run">
      <h3>
        Barocas, S., Nissenbaum, H., “Big Data’s End Run around Anonymity and
        Consent”, in Privacy, Big Data, and the Public Good Frameworks for
        Engagement”:
      </h3>
      <p>
        This reading basically addresses the issues of Big Data (tracking,
        discrimination...) Defined in the previous reading:
        <a href="#privacy_and_information_technology"
          >Privacy and Information Technology</a
        >.<br />
        Choses a definition of <a href="#big_data">Big Data</a> and
        <a href="#privacy">Privacy</a> to disambiguate. <br />
        Defines briefly the common means that have been used until now to
        protect us from those issues:
        <a href="#anoniminity_and_consent">anoniminity and consent</a>.<br />
        To go later further in depth about them, individually:
        <a href="#anoniminity">Anoniminity</a>, <a href="#consent">Consent</a>.
        <br />
        Showing their limits and fallacies in both theoretical and practical
        scenarios. Arguing that in many contexts they prove to be incapable of
        protecting the individual.
        <br />
      </p>
      <ul>
        <li>
          Anoniminity is rendered almost useless in cases of:
          <a href="#anonymous_identifiers">shared anonymous identifiers</a>,
          <a href="#comprehensiveness">comprehensiveness of the records</a>
          and <a href="#inference">data inference</a>.
        </li>
        <li>
          Consent is irrelevant due to the
          <a href="#transparency_paradox">transparency paradox</a>, the
          <a href="#nature_of_flows">nature of informational flows</a> in the
          big data and the
          <a href="#tiranny_of_the_minority">tiranny of the minority</a>.
        </li>
      </ul>
      <p>
        In conclusion we argue that background and context-driven rights and
        obligations are necessary to guarantee privacy and that Anonimity and
        Consent are useless without them.
      </p>
      <ul>
        <li>
          <h4>Advantages of Big Data</h4>
          <p>
            Big data promises to deliver analytic insights that will add to the
            stock of scientific and social scientific knowledge, significantly
            improve decision making in both the public and private sector, and
            greatly enhance individual self-knowledge and understanding.
          </p>
        </li>
        <li>
          <h4>Issues of Big Data</h4>
          <p>
            Big data is a threat to fundamental values, including everything
            from autonomy, to fairness, justice, due process, property,
            solidarity, and, perhaps most of all, privacy
          </p>
        </li>
        <li>
          <h4 id="anoniminity_and_consent">
            Anonymity and consent as the solution
          </h4>
          <p>
            anonymization seems to take data outside the scope of privacy, as it
            no longer maps onto identifiable subjects, while allowing
            information subjects to give or withold consent, giving them (in
            theory) control over the information they want to share. This fits
            perfectly the common conception of privacy as: "control over
            information about oneself".
          </p>
        </li>
        <li>
          <h4>Limits of anonymity and consent</h4>
          <p>
            In practice, however, anonymity and consent have proven elusive, as
            time and again critics have revealed fundamental problems in
            implementing both.
          </p>
        </li>
        <li>
          <h4>Virtually intractable challenge to anonymity</h4>
          <p>
            Even when individuals are not ‘identifiable’, they may still be
            ‘reachable’, may still be comprehensibly represented in records that
            detail their attributes and activities, and may be subject to
            consequential inferences and predictions taken on that basis.
          </p>
        </li>
        <li>
          <h4>Virtually intractable challenge to consent</h4>
          <p>
            It is absurd to think that notice and consent can fully specify the
            terms of consent between data collector and data subject. This
            consideration highligts the inefficacy of consent as a matter of
            individual choice.
          </p>
        </li>
        <li id="big_data">
          <h4>Definition of Big Data</h4>
          <p>
            Big Data is a belief in the power of finely observed patterns,
            structures, and models drawn inductively from massive datasets.
          </p>
        </li>
        <li id="privacy">
          <h4>Definition of privacy (as contextual integrity)</h4>
          <p>
            Privacy is the requirement that information about people (‘personal
            information’) flows appropriately, where appropriateness means in
            accordance with
            <a href="#informationsl_norms">informational norms</a>.
          </p>
        </li>
        <li id="informationsl_norms">
          <h4>Informational norms (Theory of contextual integrity)</h4>
          <p>
            Informational norms prescribe information flows according to key
            actors, types of information, and constraints under which flow
            occurs (‘transmission principles’).
          </p>
          <h5>Example</h5>
          <p>
            informational norms for a health care context would govern flow
            between and about people in their context-specific capacities, such
            as physicians, patients, nurses, insurance companies, pharmacists,
            and so forth. Types of information would range over relevant fields,
            including, say, symptoms, diagnoses, prescriptions, as well as
            biographical information. And notable among transmission principles,
            confidentiality is likely to be a prominent constraint on the terms
            under which information types flow from, say, patients to
            physicians.
          </p>
          <h5>Key concept:</h5>
          <p>
            control over information about oneself is not presumed unless the
            other parameters – (context specific) actors and information types –
            warrant it. E.G. If confidentiality is not guaranteed, it must not
            be assumed, hence if violated it does not violate Privacy, because
            information still flows accordingly to informational norms.
          </p>
        </li>
        <li>
          <h4>Why privacy as contextual integrity?</h4>
          <p>
            Because computing and information technologies have been radically
            disruptive. To distinguish between technologies that are and are not
            distruptive, a norm-based account of privacy, such as contextual
            integrity, must offer a basis for drawing such distinctions.<br />
            By keeping in view connections with specific information flows,
            certain options become salient that might otherwise not have been:
            as soon as we notice a distruption, we can study the flow to see
            what went wrong and caused a disruption.
          </p>
        </li>
        <li id="anoniminity">
          <h4>Anonimity and our assumptions about it</h4>
          <p>
            Anonymity obliterates the link between data and a specific person
            not so much to protect privacy but, in a sense, to bypass it
            entirely. <br />
            If anoniminity can be effectively guaranteed is outside of the scope
            of the extract, we assume that the problem of anonymization,
            classically speaking, has been solved (even if it has obviously
            not). <br />
            Basically, assuming that anoniminity works, is it worth it to
            protect us using it, or is it useless?
          </p>
        </li>
        <li>
          <h4>The value of anoniminity</h4>
          <p>
            The value of anonymity inheres to something we called
            ‘reachability’, the possibility of knocking on your door, holding
            you accountable – with or without access to identifying information.
          </p>
        </li>
        <li id="anonymous_identifiers">
          <h4>Anonymous Identifiers</h4>
          <p>
            Unique persistent identifiers that differ from those in common and
            everyday use. They (in theory) avoid association with records of
            other istitutions, but you are still identified within the
            institution that gave you that "Anonymous Identifier". <br />
            In practice any unique identifier or sufficiently unique pattern can
            serve as the basis for recognizing the same person in and across
            multiple databases, so they are not so good. The protective value of
            Unique persistent identifiers decreases as they are adopted by or
            shared with additional institutions (E.G. Social Security Number).
          </p>
        </li>
        <li id="comprehensiveness">
          <h4>Comprehensiveness</h4>
          <p>
            A further worry is that the comprehensiveness of the records
            maintained by especially large institutions – records that contain
            no identifying information – may become so rich that they subvert
            the very meaning of anonymity.<br />
            “if a company knows 100 data points about me in the digital
            environment, and that affects how that company treats me in the
            digital world, what’s the difference if they know my name or not?” -
            Turow<br />
            We can no longer turn to anonymity (or, more accurately,
            pseudonymity) to pull datasets outside the remit of privacy
            regulations and debate.
          </p>
        </li>
        <li id="inference">
          <h4>Inference</h4>
          <p>
            Even without names, personal information or anonymous identifiers,
            thanks to data mining a lot can be inferred from a given big enough
            data set. This renders the traditional protections afforded by
            anonymity (again, more accurately, pseudonymity) much less
            effective.
          </p>
          <h5>Example</h5>
          <p>
            Rather than attempt to de-anonymize medical records, for instance,
            an attacker (or commercial actor) might instead infer a rule that
            relates a string of more easily observable or accessible indicators
            to a specific medical condition.
          </p>
        </li>
        <li>
          <h4>Research Underwritten by Anonymity</h4>
          <p>
            Findings from research on sone attributes based on anonymous data
            may provide institutions with new paths by which to infer precisely
            those attributes that were previously impossible to associate with
            specific individuals in the absence of identifying information.
            <br />
            Basically a research meant to guarantee anoniminty enables
            institutions to infer characteristics that were previously
            "protected" by anoniminity. They should have discussed about that
            beforehand.
          </p>
        </li>
        <li id="consent">
          <h4>Informed consent</h4>
          <p>
            Where anonymity is unachievable or simply does not make sense,
            informed consent often is the mechanism sought out by conscientious
            collectors and users of personal information.
          </p>
        </li>
        <li>
          <h4>Fair Information Practice Principles (FIPPs)</h4>
          <p>
            Theese are the first principles of informed consent in privacy law.
            These principles, in broad brushstrokes, demand that data subjects
            be given notice, that is to say, informed who is collecting, what is
            being collected, how information is being used and shared, and
            whether information collection is voluntary or required. The
            Internet challenged them a lot, eventually coming up with privacy
            policies.
          </p>
        </li>
        <li>
          <h4>Privacy Policies (How consent is handled in the Internet)</h4>
          <p>
            Over the course of roughly a decade and a half, privacy policies
            have remained the linchpin of privacy protection online, despite
            overwhelming evidence that most of us neither read nor understand
            them. The debate on how to make them better is still active (opt in
            vs opt out, ...).
          </p>
        </li>
        <li>
          <h4>Assumptions about consent</h4>
          <p>
            We accept that informed consent is a useful privacy measure in
            certain circumstances and against certain threats and that existing
            mechanisms can and should be improved, but, against the challenges
            of big data, consent, by itself, has little traction. (much like the
            approach taken on
            <a href="#anoniminity">Anonimity</a>)
          </p>
        </li>
        <li id="transparency_paradox">
          <h4>The Transparency Paradox</h4>
          <p>
            Simplicity and clarity unavoidably results in losses of fidelity.
            <br />
            A few simple and clear statments cannot specify all the implications
            of consenting. <br />Even the few people want to go in detail and
            actually read the policies, will most likely not understand them.
            <br />And even if they do understand, the information given to them
            will be so general that they still will not – indeed cannot – be
            informed in ways relevant to their decisions whether to consent.
          </p>
          <h5>Example</h5>
          <p>
            What can it mean to an ordinary person that the information will be
            shared with Axciom or Choicepoint, let alone the NSA? <br />
            Characterizing the type of information is even tougher. Is it
            sufficient for the utility company to inform customers that it is
            collecting smart meter readings?
          </p>
        </li>
        <li id="nature_of_flows">
          <h4>Indeterminate, Unending, Unpredictable data flows and usage</h4>
          <p>
            Deciding how to describe information practices in ways that are
            relevant to privacy so that individuals meaningfully grant or
            withhold consent is difficult due to the machinations of big data:
            data moves from place to place and recipient to recipient in
            unpredictable ways. <br />
            Who will ask to buy that data (if at all)? <br />
            To which data will this data be aggregated?. <br />
            Basically, the flows of the
            <a href="informational_norms">informational noms</a> are not well
            defined unless recipients and transmission principles are specified
            (which is almost never the case). <br />
            As many have now argued, consent under those conditions is not
            meaningful.
          </p>
        </li>
        <li>
          <h4>How to consent for correlated data?</h4>
          <p>
            What if from a large dataset, using data mining techniques, some
            unanticipated correlations are discovered? <br />Is it the data
            controller's responsibility to warn the informing subjects about
            such possibility? <br />How should he inform them, if even he does
            not know the possible findings in advance? <br />
            As many have now argued, consent under those conditions is not
            meaningful.
          </p>
        </li>
        <li id="tiranny_of_the_minority">
          <h4>The tiranny of the minority</h4>
          <p>
            The volunteered information of the few can unlock the same
            information about the many. <br />
            "it’s no longer about what you do that will go down on your
            permanent record. Everything that everyone else does that concerns
            you, implicates you, or might influence you will go down on your
            permanent record."" - Hence danah. <br />
            It is even possible to make inferences about people who are not even
            a part of an online social network (i.e. to learn things about
            obviously absent nonmembers). <br />
          </p>
          <h5>Examples</h5>
          <p>
            1) Let's say that 10 people near to you consent to share their data,
            from that data a certain pattern is extracted E.G. those people
            switch from WhatsApp to Telegram. It is now possible to infer that
            you are likely switch from WhatsApp to Telegram.<br />
            2) More generally: Target was able to infer a rule about the
            relationship between purchases and pregnancy from what must have
            been a tiny proportion of all its customers who actually decided to
            tell the company that they recently had a baby.
          </p>
          <h5>Consequences</h5>
          <p>
            Multiple attributes can be inferred globally when as few as 20% of
            the users reveal their attribute information. <br />
            Once a critical threshold has been reached, data collectors can rely
            on more easily observable information to situate all individuals
            according to these patterns, rendering irrelevant whether or not
            those individuals have consented to allowing access to the critical
            information in question. Withholding consent will make no difference
            to how they are treated!
          </p>
        </li>
        <li>
          <h4>Points of view</h4>
          <p>
            1) Some people think that privacy and big data are incompatible, to
            them, the points shown before just enforce the need to dislodge
            privacy from its pedestal and allowthe glorious potential of big
            data to be fulfilled. <br />
            2) Others say that we should remain concerned about ethical issues
            raised by big data, that, while privacy may be a lost cause, the
            real problems arise with use. Basiccally allow data collection and
            prevent misuse. <br />
            3) Our point of view: we are not yet readyto give up on privacy, nor
            completely on anonymity and consent.
          </p>
        </li>
        <li>
          <h4>Our conclusions:</h4>
          <p>
            Background and context-driven rights and obligations have been
            neglected in favor of anonymity and consent to the detriment of
            individuals and social integrity. They should instead be the
            foundation of how privacy is guaranteed: <br />
            0) There should be a set of background and context driven rights and
            obligations that must be respected. <br />
            1) The user should only consent to data collection or sharing that
            violates this set of obligations <br />
            2) This sharing should be very limited in time and purpose <br />
            3) It is up to the data user to justify why certain data shall be
            collected. <br />
            4) Where, for example, anonymizing data, adopting pseudonyms, or
            granting or withholding consent makes no difference to outcomes for
            an individual, we had better be sure that the outcomes in question
            can be defended as morally and politically legitimate.
            <br />
            5) When anonymity and consent do make a difference, we learn from
            the domain of scientific integrity that simply because someone is
            anonymous or pseudonymous or has consented does not by itself
            legitimate the action in question.
            <br />
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3 id="why_software_should_be_free">
        Stallman, R. (1995). “Why software should be free”. Computer Ethics and
        Social Values, Johnson, D. and Nissenbaum, H. (eds.), Prentice Hall,
        190-199:
      </h3>
    </section>
    <h3>Exam Dates</h3>
    <ul class="types-list">
      <li>21 DEC</li>
      <li>22 JAN</li>
      <li>15 FEB</li>
      <!-- Add more exam dates as needed -->
    </ul>
  </body>

  <script>
    function toggleCompress(el) {
      console.log(el);
      if (!el.parentElement.classList.contains("compressed")) {
        el.parentElement.classList.add("compressed");
        return;
      }
      if (!el.parentElement.classList.contains("minimized")) {
        el.parentElement.classList.add("minimized");
        return;
      }
      el.parentElement.classList.remove("minimized");
      el.parentElement.classList.remove("compressed");
    }

    document
      .querySelectorAll("section h3")
      .forEach((el) => el.addEventListener("click", () => toggleCompress(el)));
    document
      .querySelectorAll("section")
      .forEach((el) => el.classList.add("compressed", "minimized"));
  </script>
</html>
