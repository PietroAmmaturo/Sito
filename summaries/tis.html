<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Computer Ethics</title>
  </head>
  <style>
    html {
      font-family: Arial, sans-serif;
      scroll-behavior: smooth;
      font-size: 2em;
      margin: 1em;
    }

    section.minimized ul {
      display: none;
    }
    section.minimized p {
      display: none;
    }
    section.minimized h4 {
      display: none;
    }
    section.compressed li p {
      display: none;
    }
    section.compressed li h5 {
      display: none;
    }
    section.compressed li ul {
      display: none;
    }
    section.compressed table {
      display: none;
    }
  </style>

  <body>
    <h1>Computer Ethics</h1>
    <h2 onclick="toggleCompress()">Topics, click to show</h2>
    <section>
      <h3>Intro to data integration</h3>
      <p></p>
      <ul>
        <li>
          <h4 id="what_is_ce">The data integration problem</h4>
          <p>
            Combining data coming from different data sources, providing the
            user with a unified vision of the data.
          </p>
        </li>
        <li>
          <h4>The four V’s of Big data in data integration</h4>
          <p>
            <a href="#volume">Volume</a>, <a href="#velocity">Velocity</a>,
            <a href="#variety">Variety</a>,<a href="#veracity">Veracity</a>.
          </p>
        </li>
        <li>
          <h4 id="volume">Volume</h4>
          <p>
            Not only can each data source contain a huge volume of data, but
            also the number of data sources has grown to be in the millions.
          </p>
        </li>
        <li>
          <h4 id="velocity">Velocity</h4>
          <p>
            As a direct consequence of the rate at which data is being collected
            and continuously made available, many of the data sources are very
            dynamic.
          </p>
        </li>
        <li>
          <h4 id="variety">Variety (heterogeneity)</h4>
          <p>
            People and enterprises need to integrate data and the systems that
            handle those data: relational DBMSs and their extensions, legacy
            data and legacy DBMSs, sensors and user-generated content produce
            heterogeneous, structured or unstructured data.
          </p>
          <h5>Causes</h5>
          <p>
            1. Different platforms: Technological heterogeneity <br />
            2. Different data models of the participating datasets à Model
            heterogeneity<br />
            3. Different query languages -> Language heterogeneity<br />
            4. Different data schemas and different conceptual representations
            in DBs<br />
            previously developed à Schema (semantic) heterogeneity<br />
            5. Different values for the same info (due to errors or to
            different<br />
            knowledge)à Instance (semantic) heterogeneity
          </p>
        </li>
        <li>
          <h4 id="veracity">Veracity (Data Quality)</h4>
          <p>
            Data sources (even in the same domain) are of widely differing
            qualities: <br />
            1) Completeness: is it complete?<br />
            2) Validity: is it true? <br />
            3) Consistency: is it not contraddicctory? <br />
            4) Timeliness: is it up to date? (valid now) <br />
            5) Accuracy: how specific?<br />
            6) Ethics and fairness <br />
          </p>
        </li>
        <li>
          <h4 id="data_integration_steps">Steps of data integration</h4>
          <p>
            1) Schema reconciliation: mapping the data structure (if it exists!)
            <br />
            2) Record linkage (aka Entity resolution): data matching based on
            the same content <br />
            3) Data fusion: reconciliation of non-identical content
          </p>
        </li>
        <li>
          <h4>Relevant Ways of Integrating Database Systems</h4>
          <p>
            1. Use a materialized database (data are merged in a new database):
            ETL + DW <br />
            2. Use a virtual non-materialized data base (data remain at
            sources): <br />
            o Enteprise Information Integration (EII) (or Data Integration)
            Systems (common front-end to the various datasources) <br />
            o Data Exchange (source-to-target), less used recently
          </p>
        </li>
        <li>
          <h4>Materialized vs virtual</h4>
          <p>
            Materialized is better when we want it fast and we don't need really
            up to date data. We will use these for DW. (e.g. I need a DB which
            allows me to study the trend of my company in the last 10 years).
            <br />
            Virtual is better in case of dynamic sources, in case data in the
            sources changes often (e.g. booking.com), or in case sources are
            added or removed often.
          </p>
        </li>
        <li>
          <h4 id="data_layer">
            General framework for virtual Data Integration
          </h4>
          <p>
            We are not speaking only about data integration for relational
            databases but for everything. <br />
            There are a few steps to be done: <br />
            Question route: application -> middelware -> data layer -> sources
            <br />
            1) The middleware divides the query in multiple queries one for each
            system (but not translating the language). <br />
            2) The data layer takes care of speaking to the sources in their
            languages (XML, JSON, SQL...) <br />
            Answer route: sources -> data layer -> middleware -> application
            <br />
            3) The data layer translates the answeres. <br />
            4) The middleware translates back the queries.
          </p>
        </li>
        <li>
          <h4>Middleware</h4>
          <p>
            When a query is submitted, the integration system (and specifically
            its middleware) has to decompose it into queries against the
            component datasets: <br />
            1. determine first which parts of the query refer to which dataset.
            <br />
            2. which parts apply to only a single dataset. (evaluated within the
            component datasets) <br />
            3. which parts apply to data from different datasets. (evaluated
            over the integrated data)<br />
          </p>
        </li>
        <li>
          <h4>Why data integration even in a UNIQUE DB ?</h4>
          <p>
            • Each area of the company will ask the designer to design their
            (part of) database (DB) <br />
            • However, a lot of this data are common to some of the areas
            <br />
            • If the global company DB is just the collection of these partial
            DBs, there will be many redundancies (useless memory occupation)<br />
            • Worst of all, when updating one instance of these duplicates,
            maybe the other one will remain as before (inconsistencies)
          </p>
          <h5>Solution</h5>
          <p>
            Within a single company, there shall be an integrated, centralized
            DB. <br />
            This ELIMINATES useless REDUNDANCIES which would cause: <br />
            • Inconsistencies (i.e. contradictions) <br />
            • Useless memory occupation <br />
            Each functionality in the company will have its own personalized
            view. <br />
            NB: even if decentrlized, it should be built as centralized and then
            decentralized.
          </p>
        </li>
        <li>
          <h4>Types of sources to be integrated</h4>
          <p>
            • Federated DBs <br />
            o Homogeneous data: same data model <br />
            o Heterogeneous data: different data models: OO, XML, relational,
            RDF, Web data etc. (Semi-structured data), even free text
            (unstructured data) <br />
            • Transient, initially unknown data sources <br />
          </p>
        </li>
        <li>
          <h4>Multidatabase</h4>
          <p>
            We must build a system that: <br />
            • Supports access to different data sources <br />
            • “Knows” the contents of these data sources <br />
            • Integrates the different data sources by means of a unifying,
            global schema <br />
            • Receives queries expressed in the language of the global schema
            <br />
            • Distributes “rewritten” queries to the sources <br />
            • Combines the answers received from the sources to build the final
            answer <br />
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>Structured data integration</h3>
      <ul>
        <li>
          <h4>
            Designing data integration in the Multidatabase (with a unique data
            model)
          </h4>
          <p>
            1. Source schema identification (when present) <br />
            2. Source schema reverse engineering (data source conceptual
            schemata) <br />
            3. Conceptual schemata integration and restructuring: conflict
            resolution and restructuring <br />
            4. Conceptual to logical translation (of the obtained global schema)
            <br />
            5. Mapping between the global logical schema and the single schemata
            (logical view definition) <br />
            6. After integration: query-answering through data views <br />
          </p>
        </li>
        <li>
          <h4 id="logical_view_definition">Logical view definition</h4>
          <p>
            Mapping between the global logical (mediated) schema and the single
            source schemata. <br />
            • Two basic techniques: <a href="#GAV">GAV</a>,
            <a href="#LAV">LAV</a>.
          </p>
        </li>
        <li>
          <h4 id="GAV">GAV (Global As View)</h4>
          <p>
            • Up to now we supposed that the global schema be derived from the
            integration process of the data source schemata <br />
            • Thus the global schema is expressed in terms of the data source
            schemata <br />
            • Such approach is called the Global As View approach
          </p>
          <h5>GAV mapping</h5>
          <p>
            • A GAV mapping is a set of assertions, one for each element g of G
            a <br />
            g -> qS <br />
            That is, the mapping specifies g as a query qS over the data
            sources. This means that the mapping (view) tells us exactly how the
            element g is computed.
          </p>
          <h5>When?</h5>
          <p>
            1) OK for stable data sources <br />
            2) Difficult to extend with a new data source
          </p>
          <h5>Considerations</h5>
          <p>
            • Suppose we introduce a new source <br />
            • The simple views we have just created must be modified <br />
            • In the simplest case (like in the GLOB_PROD case) we only need to
            add a union with a new SELECT FROM-WHERE clause <br />
            • This is not true in general, view definitions may be much more
            complex <br />
            • In LAV this problem does not occur
          </p>
        </li>
        <li>
          <h4 id="LAV">LAV (Global As View)</h4>
          <p>
            • The global schema has been designed independently of the data
            source schemata <br />
            • The relationship (mapping) between sources and global schema is
            obtained by defining each data source as a view over the global
            schema
          </p>
          <h5>LAV mapping</h5>
          <p>
            A mapping LAV is a set of assertions, one for each element s of each
            source S <br />
            s à qG <br />
            Thus the content of each source is characterized in terms of a view
            qG over the global schema
          </p>
          <h5>When?</h5>
          <p>
            1) OK if the global schema is stable, e.g. based on a domain
            description (e.g. an ontology) or an enterprise model <br />
            2) It favours extensibility <br />
            3) Query processing much more complex
          </p>
          <h5>Considerations</h5>
          <p>
            • Some information may be lacking: there is no corresponding value.
            <br />
            • On the other hand, no query coming from the global schema will
            ever require those attributes, since they are not present in the
            global schema
          </p>
        </li>
        <li>
          <h4>How to handle inconsistencies</h4>
          <p>
            At query processing time, when a real-world object is represented by
            instances in different databases, they may have different values. To
            handle theese inconsistencies there are two ways:
            <a href="#record_linkage">Record Linkage</a>,
            <a href="#data_fusion">Data Fusion</a>.
          </p>
        </li>
        <li>
          <h4 id="#record_linkage">Record Linkage (Entity Resolution)</h4>
          <p>
            Whatever the data model, we have to recognize when two datasets
            contain the same information. There are 2 main ways to do Record
            Linkage: <a href="#similarity_measures">similarity measures</a> and
            <a href="#record_matching">record matching</a>.
          </p>
          <h5>Example</h5>
          <p>
            If 2 DB contain the same touple, but in one the address is saved as
            string, in another there is no adress column, but there are city,
            street and number.
          </p>
        </li>
        <li>
          <h4 href="#similarity_measures">Similarity Measures</h4>
          <p>
            The higher the similarity the more similar are the strings. Types of
            similarity measures: <br />
            1) Sequence-based: edit distance, Needleman-Wunch, affine gap, Smith
            Waterman, Jaro, Jaro-Winkler <br />
            2) Set-based: overlap, Jaccard, TF/IDF <br />
            3) Hybrid: generalized Jaccard, soft TF/IDF, Monge-Elkan <br />
            4) Phonetic: Soundex
          </p>
          <h5>Efficiency</h5>
          <p>
            Applying s(x,y) to all pairs is quadratic in the size of the
            datasets, which is usually bad.
          </p>
        </li>
        <li>
          <h4 href="#edit_distance">Edit Distance</h4>
          <p>
            The edit distance is based on the minimal number of operations that
            are needed to transform string a into string b.
          </p>
        </li>
        <li>
          <h4 href="#set_based">Set-based (Specifically Jaccard)</h4>
          <p>
            View strings as multisets of tokens. <br />
            • For strings, this corresponds to dividing the strings into tokens,
            and computing the measure on the two sets of tokens.
          </p>
          <h5>Example</h5>
          <p>
            The tokens are pieces of the strings, like for instance, we can
            choose tokens of lenght 2: <br />
            E.g., for strings pino and pin <br />
            A = {#p, pi, in, no, o#}, B = {#p, pi, in, n#} <br />
            J(pino, pin) = 3/(5+4)-3 = 3/6 = 1/2
          </p>
        </li>
        <li>
          <h4 href="#phonetic">Phonetic Similarity Measures</h4>
          <p>
            • Match strings based on their sound <br />
            • Soundex is the most common one. Soundex calculates a
            four-character code from a word based on the pronunciation and
            considers two words as similar if their codes are equal.
          </p>
        </li>
        <li>
          <h4 id="record_matching">Record Matching</h4>
          <p>
            1) <a href="#rule">Rule-based</a> <br />
            2) <a href="#learning">Learning-based</a> <br />
            3) <a href="#probabilistic">Probabilistic</a> <br />
          </p>
        </li>
        <li>
          <h4 id="rule">Rule Matching</h4>
          <p>
            Manually written rules that specify when two tuples match. E.g. two
            tuples refer to the same person if they have the same SSN
          </p>
        </li>
        <li>
          <h4 id="learning">Learning Matching</h4>
          <p>Learning can be supervised or unsupervised <br /></p>
          <h5>Supervised</h5>
          <p>
            Supervised (e.g. classification): learn how to match from training
            data, then apply it to match new tuple pairs: <br />
            1) Learn how to match each attribute of the tuples (the training
            phase): each (xi ,yi ) is a pair of elements and li is a label: “
            yes ” if xi matches yi and “ no ” otherwise <br />
            2) Define the weight of each attribute in the final matching of the
            whole records<br />
            3) Apply the learned model to the new tuple pairs<br />
            NB: It requires lots of training data
          </p>
          <h5>Unsupervised</h5>
          <p>
            Unsupervised learning (typically clustering, based on clusterizing
            similar values) may solve this problem.
          </p>
        </li>
        <li>
          <h4 id="probabilistic">Probabilistic Matching</h4>
          <p>
            • Model the matching domain using a probability distribution <br />
            • Reason with the distribution to make matching decisions
          </p>
          <h5>Benefits</h5>
          <p>
            o provide a principled framework that can naturally incorporate a
            variety of domain knowledge <br />
            o can leverage the wealth of probabilistic representation and
            reasoning techniques already developed in the AI and DB communities
            <br />
            o provide a frame of reference for comparing and explaining other
            matching approaches
          </p>
          <h5>Disadvantages</h5>
          <p>
            o computationally expensive <br />
            o often hard to understand and debug matching decisions
          </p>
        </li>
        <li>
          <h4>Data Fusion</h4>
          <p>
            Given that some data clearly represent the same entity, what to do
            when other parts of the info do not match. How we decide is
            extremely dependant on the application case: a
            <a href="#resolution">resolution function</a> must be chosen.
          </p>
        </li>
        <li>
          <h4 id="#resolution_function">Resolution Function</h4>
          <p>
            Inconsistency may depend on different reasons: o One (or both) of
            the sources are incorrect<br />
            o Each source has a correct but partial view, e.g. databases from
            different workplaces, e.g.: <br />
            • the full salary is the sum of the two <br />
            • the list of authors of a book is incomplete <br />
            • one of two full names only contains the first letter of the middle
            name <br />
            o Often, the correct value may be obtained as a function of the
            original ones, e.g. : value1 + value2 , or 1*value1 + 0*value2 or
            0,5*value1 + 0,5*value2
          </p>
        </li>
        <li>
          <h4>Levels of source heterogeneity</h4>
          <p>
            • Schemata (
            <a href="#logical_view_definition">Logical View Definition</a>
            ): Externam Schemata -> Data Integration -> Component Schemata
            <br />
            • Data Models (<a href="#wrappers">Wrappers</a>) <br />
            • Systems (easily overcome if we reconcile data models and schemata)
          </p>
        </li>
        <li>
          <h4 id="wrappers">Wrappers (translators)</h4>
          <p>
            • Wrappers basically implement the
            <a href="#data_layer">data layer</a> of the global figure: they
            convert queries into queries/commands which are understandable for
            the specific data source, possibly extending the query possibilities
            of a data source (see e.g. type conversions, celsius to far) <br />
            • They convert query results from the source format to a format
            which is understandable for the query/application <br />
            • Easy to produce in the case of structured data: e.g. for the
            relational model and the Object Oriented model, we can translate the
            queries and the results between the two models <br />
            • More difficult is the problem if the data is not structured
            (later)
          </p>
        </li>
        <li>
          <h4>Design steps</h4>
          <p>
            1. Reverse engineering (i.e. production of the conceptual schema)
            <br />
            2. Conceptual schemata integration <br />
            3. Choice of the target logical data model and translation of the
            global conceptual schema <br />
            4. Definition of the language translation (wrapping) <br />
            5. Definition of the data views (as usual)
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>Semistructured data integration</h3>
      <ul>
        <li>
          <h4>Semistructured data</h4>
          <p>
            For this data there is some form of structure, but it is not as:
            <br />
            1) Presriptive <br />
            2) Regular <br />
            3) Complete <br />
            As in traditional DBMSs
          </p>
          <h4>Examples</h4>
          <p>
            - Web data <br />
            - XML sata <br />
            - Data derived from the integration of hetereogeneous datasources.
          </p>
        </li>
        <li>
          <h4>Semistructured data models</h4>
          <p>
            Based on: <br />
            1) Text <br />
            2) Trees <br />
            3) Graphs <br />
            - Labeled nodes <br />
            - Labeled arcs <br />
            - Both
          </p>
        </li>
        <li>
          <h4>Information search in semistructured DBs</h4>
          <p>
            An overall data representation should be progressively built, as we
            discover and explore new information sources. GAV and LAV are no
            more sufficient, we need <a href="#mediators">Mediators</a>.
          </p>
        </li>
        <li>
          <h4 id="mediators">Mediators</h4>
          <p>
            Mediators must do many different things: <br />
            1) the processing needed to make the interfaces work <br />
            2) the knowledge structures that drive the transformations needed to
            transform data to information <br />
            3) any intermediate storage that is needed (Wiederhold) <br />
          </p>
          <h5>Problem</h5>
          <p>
            The mediator has to be an expert of the domain, hence each different
            domain needs a mediator appropriately designed to “understand” its
            semantics.
          </p>
        </li>
        <li>
          <h4>Mediation-based systems</h4>
          <p>
            1) The mediator will understand the query, to which sources send the
            query and reconcile the schemas. <br />
            2) The wrapper translates from the mediator language to the specific
            data sources language.
          </p>
          <h5>Example: TSIMMIS</h5>
          <p>
            1) Unique, graph-based internal data model: <a href="#oem">OEM</a>,
            managed by the mediator. <br />
            2) Wrappers for the model-to-model translations. <br />
            3) Query posed to the mediator in the
            <a href="#lorel">LOREL</a> language. <br />
            4) Mediator "knows" the semantics of the application domain. 5) It
            introduced the <a href="#dataguide">Dataguide</a>.
          </p>
        </li>
        <li>
          <h4 id="oem">OEM</h4>
          <p>
            1) Graph-based <br />
            2) It does not represent the schema <br />
            3) It directly represents the data : self-descriptive <br />
            "<(Object-id),label,type,value>"
          </p>
          <h5>Example</h5>
          <p>
            "temp-in-farenheit,int,80>" where 80 is the data, the other 2 are
            used to understand the information.
          </p>
        </li>
        <li>
          <h4 id="lorel">LOREL</h4>
          <p>
            1) Lightweight Object REpository Language <br />
            2) Object-based <br />
            3) Similar to object oriented query languages, with some
            modifications appropriate for semistructured data
          </p>
          <h5>Example</h5>
          <p>
            “ Find books authored by Aho” <br />
            select library.book.title <br />
            where library.book.author = “Aho” <br />
            from library
          </p>
        </li>
        <li>
          <h4 id="dataguide">Dataguide</h4>
          <p>
            The TSIMMIS system introduced the Dataguide: a kind of a-posteriori
            schema, progressively built by the Mediator while exploring the data
            sources. <br />
            Is a guide to give to the people and t the application, to be read
            by the people, so that they can write the queries in an intelligent
            way. <br />
          </p>
          <h5>Example</h5>
          <p>
            It tells the user (or the system, if a transformation has to be
            applied) that a node library exists, which contains nodes book,
            which in turn contain the fields author and title. Otherwise the
            user has no clue.
          </p>
        </li>
        <li>
          <h4>
            Typical complications when integrating semi- or un-structured data
          </h4>
          <p>
            1) Each mediator is specialized into a certain domain (e.g. weather
            forecast), thus <br />
            2) Each mediator must know domain metadata , which convey the data
            semantics <br />
            3) On-line duplicate recognition, reconciliation and removal (no
            designer to solve conflicts at design time here) <br />
            4) If data source changes a little, the wrapper has to be modified à
            automatic wrapper generation (later)
          </p>
        </li>
        <li>
          <h4>Wrappers and unstructured data</h4>
          <p>
            1) In the case of unstructured data the wrapper must translate from
            an unstructured language to a structured one and viceversa. <br />
            2) This process is much easier if the unstructured content is
            regular (e.g. derived from a DB). <br />
            3) Even do, the unstructured content might change, making the
            wrapper useless. <br />
            4) Human-based maintenence of wrappers is expensive <br />
            5) We need
            <a href="automatic_wrapper">automatic wrapper generation</a>.
          </p>
        </li>
        <li>
          <h4>Automatic wrapper generation</h4>
          <p>
            We can only use it when pages are regular to some extent. Ok when
            many pages sharing the same structure.
          </p>
          <h5>Exaples</h5>
          <p>
            1) pages are dynamically generated from a DB <br />
            2) data intensive web sites
          </p>
        </li>
        <li>
          <h4>The Road Runner project</h4>
          <p>
            1) Page Class: <br />
            The collection of all pages generated by the same script from a
            common dataset <br />
            2) Schema Derivation <br />
            Given a set of HTML sample pages, belonging to the same class, find
            the underlying dataset structure (database schema) <br />
          </p>
          <h5>Solution</h5>
          <p>
            Wrapper Generator <br />
            1) Underlying dataset structure <br />
            2) Extraction rules <br />
          </p>
        </li>
        <li>
          <h4>Ontologies</h4>
          <p>
            Ontologies are a way to solve the problem of automatic semantic
            matching. <br />
            1) A formal and shared definition of a vocabulary of terms and their
            inter-relationships <br />
            2) Predefined relations: synonimy, omonimy, hyponimy, etc.. <br />
            3) More complex, designer-defined relationships, whose semantics
            depends on the domain
          </p>
          <h5>Example</h5>
          <p>
            an ER diagram, a class diagram, any conceptual schema is a kind of
            ontology
          </p>
          <h5>Definition</h5>
          <p>
            1) formal specification of a conceptualization of a shared knowledge
            domain. <br />
            2) An ontology is a controlled vocabulary that describes objects and
            the relationships between them in a formal way. <br />
            3) It has a grammar for using the terms to express something
            meaningful within a specified domain of interest. <br />
            4) The vocabulary is used to express queries and assertions.
          </p>
          <h5>Aims</h5>
          <p>
            1) A formal specification allows for use of a common vocabulary for
            automatic knowledge sharing. <br />
            2) Formally specifying a conceptualization means giving a unique
            meaning to the terms that define the knowledge about a given domain.
            <br />
            3) Shared: an ontology captures knowledge which is common, thus over
            which there is a consensus (objectivity is not an issue here).
          </p>
        </li>
        <li>
          <h4>Ontology types</h4>
          <h5>Taxonomic ontologies</h5>
          <p>
            1) Definition of concepts through terms, their hierarchical
            organization, and additional (pre-defined) relationships
            (synonymy,composition,…). <br />
            2) To provide a reference vocabulary
          </p>
          <h5>Descriptive ontologies</h5>
          <p>
            1) Definition of concepts through data structures and their
            interrelationships, <br />
            2) Provide information for “aligning” existing data structures or to
            design new, specialized ontologies (domain ontologies). <br />
            3) Closer to the database area techniques
          </p>
        </li>
        <li>
          <h4>Concepts</h4>
          <p>
            Are the building blocks of ontologies. They can be divided in:
            <br />
            1) Generic concepts, they express general world categories <br />
            2) Specific concepts, they describe a particular application domain
            (domain ontologies) <br />
            They can be defined via a formal language or in natural language.<br />
          </p>
          <h5>Relationships between concepts</h5>
          <p>
            1) Taxonomies (IS_A), <br />
            2) Meronymies (PART_OF), <br />
            3) Synonymies, homonymies, ... <br />
            4) User-defined associations,
          </p>
        </li>
        <li>
          <h4>Formal Definitions</h4>
          <p>
            O = (C, R, I, A)<br />
            O: ontology, C: concepts, R: relations, A: axioms, I: Instances
            <br />
            1) Specified in some logic-based language <br />
            2) Organized in a ISA hierarchy <br />
            3) I is an instance collection, stored in the information source
            <br />
            An ontology is (part of) a knowledge base, composed by: a
            <a href="#tbox">T-Box</a>, an <a href="#abox">A-Box</a>
          </p>
        </li>
        <li>
          <h4 id="tbox">T-Box</h4>
          <p>
            A T-Box contains all the concept and role definitions, and also
            contains all the axioms of our logical theory (e.g. “A father is a
            Man with a Child”).
          </p>
        </li>
        <li>
          <h4 id="abox">A-Box</h4>
          <p>
            An A-box contains all the basic assertions (also known as ground
            facts) of the logical theory (e.g. “Tom is a father” is represented
            as Father(Tom)). It describes the instances.
          </p>
        </li>
        <li>
          <h4>OpenCyc</h4>
          <p>
            The entire Cyc ontology contains hundreds of thousands of terms,
            along with millions of assertions relating the terms to each other,
            forming an ontology whose domain is all of human consensus reality.
          </p>
        </li>
        <li>
          <h4 id="semantic_web">Semantic Web</h4>
          <p>
            1) A vision for the future of the Web in which information is given
            explicit meaning, making it easier for machines to automatically
            process and integrate information available on the Web. <br />
            2) Built on XML's ability to define customized tagging schemes and
            RDF's flexible approach to representing data(*). <br />
            3) The first level above RDF: OWL, an ontology language what can
            formally describe the meaning of terminology used in Web documents à
            beyond the basic semantics of RDF Schema.
          </p>
        </li>
        <li>
          <h4>Linked Data</h4>
          <p>
            Linked data is a W3C backed movement about connecting data sets
            across the Web. It describes a method of publishing structured data
            so that it can be interlinked and become more useful. It builds upon
            standard web technologies (e.g. HTTP, RDF...), but extends them to
            share information in a way that can be read automatcally by
            computers, enabling data from different sources to be connected and
            queried. <br />
            It is a subset of the
            <a href="#semantic_web">Semantic Web</a> movement, which is about
            adding meaning to the web.
          </p>
        </li>
        <li>
          <h4>Open Data</h4>
          <p>
            Describes data that has been uploaded to the Web and is accessible
            to all.
          </p>
        </li>
        <li>
          <h4>Linked Open Data</h4>
          <p>
            Extend the Web with a data commons by publishing various open
            datasets as <a href="#rdf">RDF</a> on the Web and by setting RDF
            links among them
          </p>
        </li>
        <li>
          <h4 id="rdf">RDF</h4>
          <p>
            At the core of RDF is this notion of a triple
            subject-predicateobject, a statement that represents two vertices
            connected by an edge: <br />
            1) Subject: a resource, or a node in the graph <br />
            2) Predicate: an edge – a relationship <br />
            3) Object: another node or a literal value <br />
            is a data model for objects ("resources") and relations between
            them, provides a simple semantics for this data model, and can be
            represented in an XML syntax.
          </p>
        </li>
        <li>
          <h4>RDF Schema</h4>
          <p>
            Is a vocabulary for describing properties and classes of RDF
            resources, with a semantics for generalization-hierarchies of such
            properties and classes.
          </p>
        </li>
        <li>
          <h4>XML</h4>
          <p>
            XML provides a surface syntax for structured documents, but imposes
            no semantic constraints on the meaning of these documents.
          </p>
        </li>
        <li>
          <h4>XML Schema</h4>
          <p>
            XML Schema is a language for restricting the structure of XML
            documents and also extends XML with data types.
          </p>
        </li>
        <li>
          <h4>OWL</h4>
          <p>
            1) The OWL Web Ontology Language is designed for use by applications
            that need to process the content of information instead of just
            presenting information to humans. <br />
            2) OWL facilitates greater machine interpretability of Web content
            than that supported by XML, RDF, and RDF Schema (RDF-S) by providing
            additional vocabulary for describing properties and classes: among
            others, relations between classes (e.g. disjointness), cardinality
            (e.g. "exactly one"), equality, richer typing of properties,
            characteristics of properties (e.g. symmetry), and enumerated
            classes along with a formal semantics. <br />
            3) OWL has three increasingly-expressive sublanguages: OWL Lite, OWL
            DL, and OWL Full. <br />
          </p>
        </li>
        <li>
          <h4>Reasoning services for ontologies</h4>
          <p>
            There are reasoning services for ontologies, that are differentiated
            in: <br />
            <a href="#tbox_services">Services for the Tbox</a> <br />
            <a href="#abox_services">Services for the Abox</a>
          </p>
        </li>
        <li>
          <h4 id="tbox_services">Services for the Tbox</h4>
          <p>
            1) Subsumption: verifies if a concept C subsumes (is a subconcept
            of) another concept D 2) Consistency: verifies that there exists at
            least one interpretation I which satisfies the given Tbox 3) Local
            Satisfiability: verifies, for a given concept C, that there exists
            at least one interpretation in which C is true
          </p>
        </li>
        <li>
          <h4 id="abox_services">Services for the Abox</h4>
          <p>
            1) Consistency: verifies that an Abox is consistent with respect to
            a given Tbox <br />
            2) Instance Checking: verifies if a given individual x belongs to a
            particular concept C <br />
            3) Instance Retrieval: returns the extension of a given concept C,
            that is, the set of individuals belonging to C.
          </p>
        </li>
        <li>
          <h4>ER vs.ontology</h4>
          <p>
            1) Entities correspond to concepts <br />
            2) Relationships correspond to Relations <br />
            3) Hierarchies correspond to ISA <br />
            4) Attributes correspond to data type properties <br />
            Caution: An ER schema does not have VALUES !
          </p>
        </li>
        <li>
          <h4>DB vs ontologies</h4>
          <p>
            How should we improve database conceptual models to fulfill ontology
            requirements ? <br />
            1) Supporting defined concepts and adding the necessary reasoning
            mechanisms <br />
            2) Managing missing and incomplete information: semantic differences
            between the two assumptions made w.r.t. missing information (Closed
            World Assumption vs. Open World Assumption) <br />
            3) Databases are assumed to represent certain data: a tuple in the
            database is true, any tuple NOT in the database is false (Closed
            World Assumption)
          </p>
        </li>
        <li>
          <h4>Ontology matching</h4>
          <p>
            1) The process of finding pairs of resources coming from different
            ontologies which can be considered equal in meaning – matching
            operators <br />
            2) Again we need some kind of similarity measure. <br />
            3) Recall: a similarity value is usually a number in the interval
            [0,1] <br />
            4) Caution: this time the
            <a href="#similarity">similarity</a> measure takes into account
            semantics, not only on the structure of the words as seein in
            <a href="#similarity_measures"> Similarity Measures</a> !
          </p>
        </li>
        <li>
          <h4 id="similarity">Similarity</h4>
          <p>
            1) The concept of similarity is a basic concept in human cognition.
            <br />
            2) Similarity plays an essential role in taxonomy, recognition,
            case-based reasoning and many other fields. There are many aspects
            of the concept of similarity that have eluded formalization.
          </p>
        </li>
        <li>
          <h4>Ontology mapping</h4>
          <p>
            1) The process of relating similar concepts or relations of two or
            more information sources using equivalence relations or order
            relations. <br />
            2) These relations are commonly implemented in inference and
            reasoning softwares, so we can use the output ontology to perform
            complex tasks on them without extra effort.
          </p>
        </li>
        <li>
          <h4>Reasons for ontology mismatches</h4>
          <p>
            There are several reasons for ontology mismatches, both at the
            <a href="#mismatches_definition_language"
              >definition language level</a
            >
            and at the <a href="#mismatches_ontology">ontology level</a>.
          </p>
        </li>
        <li>
          <h4 id="mismatches_definition_language">
            Ontology mismatches at the definition language level
          </h4>
          <p>
            1) Syntax <br />
            2) Availability of different constructs (e.g. part-of, synonym,
            etc.) <br />
            3) Linguistic primitives’ semantics (e.g. union or intersection of
            multiple intervals) à Normalize by translating to the same language/
            paradigm
          </p>
        </li>
        <li>
          <h4 id="mismatches_ontology">
            Ontology mismatches at the ontology level
          </h4>
          <p>
            1) Scope: Two classes seem to represent the same concept, but do not
            have exactly the same instances <br />
            2) Model coverage and granularity: a mismatch in the part of the
            domain that is covered by the ontology, or the level of detail to
            which that domain is modelled. <br />
            3) Paradigm: Different paradigms can be used to represent concepts
            such as time. For example, one model might use temporal
            representations based on continuous intervals while another might
            use a representation based on discrete sets of time points. <br />
            4) Encoding <br />
            5) Concept description: e.g. a distinctions between two classes can
            be modeled using a qualifying attribute or by introducing a separate
            class, or the way in which is-a hierarchy is built <br />
            6) Homonymies <br />
            7) Synonymies
          </p>
        </li>
        <li>
          <h4>How can ontologies support integration?</h4>
          <p>
            There are several ways in which ontologies can be helpful in solving
            integration problems: <br />
            1) Discovery of “equivalent” concepts <br />
            (mapping) <br />
            2) Formal representation of these mappings <br />
            3) Reasoning on these mappings : How do we use the mappings within
            our reasoning and query-answering process?
          </p>
          <h5>An ontology as a schema integration support tool</h5>
          <p>
            1) Ontologies used to represent the semantics of schema elements (if
            the schema exists) <br />
            2) Similarities between the source ontologies guide conflict
            resolution, both at the schema level (if the schemata exist) and at
            the instance level (record linkage). <br />
          </p>
          <h5>An ontology instead of a global schema</h5>
          <p>
            1) Schema-level representation only in terms of ontologies <br />
            2) Ontology mapping, merging, etc. instead of schema integration
            <br />
            3) Integrated ontology used as a schema for querying
          </p>
          <h5>
            An ontology as a support tool for content interpretation and
            wrapping
          </h5>
          <h5>
            An ontology as a mediation support tool for content inconsistency
            detection and resolution (record linkage and data fusion)
          </h5>
        </li>
        <li>
          <h4>Ontology query processing</h4>
          <p>
            Ontologies require query languages as well, for: <br />
            1) Schema exploration (when the schema is replaced by an ontology)
            <br />
            2) Reasoning on the schema <br />
            3) Instance querying (when the instance is contained in an ontology,
            like in the Semantic Web case)
          </p>
          <h5>Example</h5>
          <p>Example of ontology query language: SPARQL (W3C)</p>
        </li>
        <li>
          <h4>Ontology query processing versus database query processing</h4>
          <p>
            When we use ontologies to interact with databases, we have to take
            care of: <br />
            1) Transformation of ontological query into the language of the
            datasource, and the other way round <br />
            2) Different semantics (CWA versus OWA) <br />
            3) What has to be processed where (e.g. push of the relational
            operators to the relational engine)
          </p>
        </li>
        <li>
          <h4>The application context</h4>
          <p>
            1) A (possibly large) number of data sources <br />
            2) Heterogeneous data sources <br />
            3) Different levels of data structure <br />
            o Databases (relational, OO…) <br />
            o Semi-structured data sources (XML, HTML, more markups …) <br />
            o Unstructured data (text, multimedia etc…) <br />
            4) Different terminologies and different operational contexts <br />
            5) Time-variant data (e.g. WEB and social media) <br />
            6) Mobile, transient data sources (e.g. sensor values) <br />
            …..as you can see, everything becomes more and more dynamic.
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>Data Integration Exercise tips</h3>
      <ul>
        <li>
          <h4>1 Reverse engeneering</h4>
          <p>
            You need to generate the ER from the Logic Schema. <br />
            1) Find the foreign keys first. <br />
            2) Put in the center the table that is referenced the most from oher
            tables. <br />
            3) Put tables that reference each other near <br />
            4) Connect tables and write cardinalities. <br />
            5) If a table represents a relationship, the relationship is likely
            N:N. <br />
            6) Foreign keys are not written, not even as attributes, it is
            implicit that: <br />
            - If the relationship is N:N there is a middle table. <br />
            - Otherwise the entity on the "1" side (that has a reference to only
            one row of the other table) contains the FK <br />
          </p>
        </li>
        <li>
          <h4>2 Schema integration</h4>
          <p>
            You need to generate the global ER from the local ERs. <br />
            1) Build the table: DB1, DB2, DB3, ..., Conflicts, Solutions. <br />
            2) Identify corresponding entities in the 2 schemas. <br />
            3) Each row shall heve the 2 corresponding entities, their conflicts
            and the relative solutions.<br />
            4) It is possible that an entity in DB1 corresponds to more than one
            in DB2, in this case join them. <br />
            5) Name conflicts: choose a suitable name between the 2. <br />
            6) Data semantic conflicts: choose the best one. (e.g. between age
            and date of birth choose age). <br />
            7) Key conflicts: create a new key from the previous 2 (unless one
            includes the other). <br />
            8) Cardinality conflict: choose the cardinality that includes both.
            e.g. 1:N + 0:1 -> 0:N. <br />
            <br />
            9) Write the ER. <br />
            <br />
            10) Write the logical schema.
          </p>
        </li>
        <li>
          <h4>3 Mapping definition (GAV) and query answering</h4>
          <p>
            1) Do the query on the global schema first, it should be as easy as
            an easy DB1 qustion. <br />
            2) Do the query on the local DBs like this: <br />
            - Write the query for each DB <br />
            - Rename the columns that nees to be renamed using the keyword "AS"
            <br />
            - Make the union of the queries 3) Do the GAV mappings last (thy
            take a lot of time): <br />
            - Use KeyGen to generate ids in case of key conflicts (e.g.
            KeyGenSale(IDSaleContract, ‘LALuxuryHouses’)). NB: Sometimes you
            don't (e.g. if you use SSN as key and SSN was present in bot DBs).
            <br />
            - Use made up functions to solve semantic conflicts (e.g. feet to
            meters)
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>Data Quality</h3>
      <ul>
        <li>
          <h4>Data Driven Management</h4>
          <p>
            Data-driven Management is characterized by the practice of
            collecting data, analyzing it, and basing decisions on insights
            derived from the information.
          </p>
          <h5>Weakpoints</h5>
          <p>
            The success of data-driven decision making depends on <br />
            1) the quality of data collected <br />
            2) the methods used to analyze data
          </p>
        </li>
        <li>
          <h4>GIGO (Garbage In – Garbage Out) Phenomenon</h4>
          <p>
            If the input data is wrong, the error propagates to the output.
            <br />
            Furtheremore, ven if it is a small error...you can have the snowball
            effect: from a small error a giant consequence.
          </p>
        </li>
        <li>
          <h4>Data preparation</h4>
          <p>
            We need an adequate architecture for analyze data and to preaparate
            it because: <br />
            1) Real-word data is often incomplete, inconsistent, and contain
            many errors… <br />
            2) Data preparation, cleaning, and transformation comprises the
            majority of the work in a data mining application (90%).
          </p>
        </li>
        <li>
          <h4>Data Quality issues</h4>
          <p>
            1) Missing values <br />
            2) Duplicate data <br />
            3) Inconsistent data <br />
            4) Outliers <br />
            5) Noise
          </p>
        </li>
        <li>
          <h4>Poor data quality causes</h4>
          <p>
            1) Missing values <br />
            2) Duplicates <br />
            3) Inconsistencies <br />
            4) Outliers <br />
            5) Noise <br />
            6) Out-of-date data
          </p>
        </li>
        <li>
          <h4>Most used objective Dimensions</h4>
          <p>
            1) Accuracy: the extent to which data are correct, reliable and
            certified <br />
            2) Completeness: the degree to which a given data collection
            includes the data describing the corresponding set of real-world
            objects <br />
            3) Consistency: the satisfaction of semantic rules defined over a
            set of data items <br />
            4) Timeliness: the extent to which data are sufficiently up-to-date
            for a task
          </p>
        </li>
        <li>
          <h4>Data Quality improvement strategies</h4>
          <p>
            <a href="#data_based">Data-based approaches</a> and
            <a href="#process_based">Process-based actions</a>.
          </p>
        </li>
        <li>
          <h4 id="data_based">Data-based approaches</h4>
          <p>
            They focus on data values and aim to identify and correct errors
            without considering the process and context in which they will be
            used.
          </p>
          <h5>Examples</h5>
          <p>
            <a href="#data_cleaning">Data cleaning</a>,
            <a href="#profiling">Profiling</a>
          </p>
        </li>
        <li>
          <h4 id="process_based">Process-based actions</h4>
          <p>
            They are activated when an error occurs and aim to discover and
            eliminate the root cause of the error.
          </p>
        </li>
        <li>
          <h4 id="data_cleaning">Data cleaning</h4>
          <p>
            Data cleaning is the process of identifying and eliminating
            inconsistencies, discrepancies and errors in data in order to
            improve quality.
          </p>
          <h5>Steps</h5>
          <p>
            1) <a href="#profiling">Data profiling</a> <br />
            2) <a href="#normalization">Standardization/ normalization</a>
            <br />
            3) <a href="#correction">Error correction </a><br />
            4) Duplicate detection
          </p>
        </li>
        <li>
          <h4 id="profiling">Profiling</h4>
          <p>
            1) Analysis of content and structure of attributes: Data type,
            domain, data distribution and variance, occurence of null values,
            uniqueness, format (e.g., mm/dd/yyyy) <br />
            2) Analysis of dependencies between attributes of a single relation:
            E.g., Functional dependencies, primary key candidates <br />
            3) Analysis of overlapping attributes from different relations:
            Redundancies, foreign keys <br />
            4) Number of missing values or wrong values • current vs.expected
            cardinality <br />
            5) frequency of null values, minimum / maximum, variance <br />
            6) Duplicates <br />
            7) Number of tuples vs. Cardinality of attribute domain
          </p>
        </li>
        <li>
          <h4 id="normalization">Data transformation and normalization</h4>
          <p>
            1) Data type conversion <br />
            2) Normalization: mapping into a common format (e.g. currency: $ ->
            €) <br />
            3) Discretization of numerical values <br />
            4) Domain-specific transformations (e.g. St. -> Street)
          </p>
        </li>
        <li>
          <h4 id="correction">Error Localization and correction</h4>
          <p>
            This activity can be seen as composed of: <br />
            1) Localization and correction of inconsistencies.<br />
            2) Localize and correction of
            <a href="#missing_data">incomplete data.</a> <br />
            3) <a href="#outlier_detection">Localization of outliers</a>.
          </p>
        </li>
        <li>
          <h4 id="missing_data">Missing data</h4>
          <p>
            Missing information on different levels <br />
            1) Instance level: values, tuples, relation fragments, ... <br />
            2) Schema level: Attributes, ...
          </p>
          <h5>Main Problems on instance level:</h5>
          <p>
            1) Treating null values: missing value or default value? <br />
            2) Data truncation and data censorization <br />
            3) Biased data, e.g. caused by null values
          </p>
          <h5>Imputing missing value</h5>
          <p>
            Unbiased estimators: stimating missing values without changing
            characteristics of existing dataset (mean, variance, ...). <br />
            Exploiting functional dependencies (e.g.: #Bedrooms -> Income)
            <br />
            Techniques from statistics (e.g. Linear regression: income = c *
            #Bedrooms)
          </p>
        </li>
        <li>
          <h4 id="outlier_detection">Outlier detection</h4>
          <p>
            An outlier is a suspicious observation that deviates too much from
            other observations.
          </p>
          <h5>Issues</h5>
          <p>
            1) Detection: distribution, „geometry“, time series <br />
            2) Interpretation: data or observation error vs. real event
          </p>
        </li>
        <li>
          <h4>Duplicate detection</h4>
          <p>
            Duplicate detection (or entity reconciliation) is the discovery of
            multiple representations of the same real-world object.
          </p>
          <h5>Issues</h5>
          <p>
            1) Identify a good similarity measure <br />
            2) Minimize the number of comparisons
          </p>
        </li>
        <li>
          <h4>Data integration</h4>
          <p>
            In case of multiple sources,
            <a href="#data_integration_steps">data integration</a> is also
            needed.
          </p>
        </li>
        <li>
          <h4>Data Quality and Data Warehouses</h4>
          <p>
            Data Quality improvement methods are also used in Data Warehouse.
          </p>
        </li>
        <li>
          <h4>Data Quality and Big Data</h4>
          <p>
            Big Data tried to overcome Data Quality issues with Data Quantity.
            But quality is still an issue.
          </p>
          <h5>Big data challenges</h5>
          <p>
            1) Diversity of data sources(Variety) Abundant data types- internal
            + external data sources Complex data structures- structured,
            semi-structured, IoT Difficult data integration - ETL and
            traditional approaches useless due to data volume and velocity
            <br />
            2) Tremendous data volume (Volume) Data quality profiling and
            assessment (collection, cleaning, and integration) is difficult to
            execute in a reasonable amount of time. <br />
            3) Timeliness of data is very short (Velocity) Data is updated
            continuously. If data is not collected and analysed in real time,
            information becomes outdated and invalid. <br />
            4) Missing standard for Data Quality (Veracity) Standards have been
            proposed for DQ of traditional data sources but not for big data.
          </p>
          <h5>To summarize: most common DQ issues in big data</h5>
          <p>
            Not integrated data <br />
            Incomplete data <br />
            Incorrect data<br />
            <a href="#data_cleaning">Data cleaning</a> have to be frequently
            performed<br />
            Inconsistent sources and issues in
            <a href="#data_integration_steps"></a>data integration<br />
            Source reliability<br />
            Data variety<br />
            Human resources: find the right competencies<br />
            Data provenance and lineage informatio should be available
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>DW Conceptual Design</h3>
      <ul>
        <li>
          <h4>Data Warehouse Design</h4>
          <p>
            1) Data Warehouses are based on the multidimensional model<br />
            2) A common conceptual model for DW does not exist <br />
            3) The Entity/Relationship model cannot be used in the DW conceptual
            design <br />
            4) Solution: <a href="#fact_schema">Fact Schema</a>
          </p>
        </li>
        <li>
          <h4 id="fact_schema">Fact Schema</h4>
          <p>
            A schema made of <a href="#fact">facts and dimensions</a>,
            <a href="#dimensional_attribute">dimensional attributes</a> which
            can be organizaed in
            <a href="#dimensional_hierarchy">dimensional hierarchies</a> which
            may also end up <a href="#convergence">converging</a> or may be
            <a href="#hierarchy_sharing">shared</a>,
            <a href="#descriptive_attribute">descriptive attributes</a>,
            <a href="#cross_dimensional_attributes"
              >cross dimensionsal attributes</a
            >.
          </p>
        </li>
        <li>
          <h4 id="fact">Fact</h4>
          <p>
            1) A fact describes an N:M relationship among its dimensions <br />
            2) There must be a functional dependency between the fact and its
            dimensions <br />
            3) A fact can have multiple measures (e.g. fact sale has: quantity,
            gross income, unitary price...) <br />
          </p>
        </li>
        <li>
          <h4 id="dimensional_attribute">Dimensional attribute</h4>
          <p>
            A dimensional attribute must assume discrete values, so that it can
            contribute to represent a dimension <br />
          </p>
        </li>
        <li>
          <h4 id="dimensional_hierarchy">Dimensional hierarchy</h4>
          <p>
            A dimensional hierarchy is a directional tree whose <br />
            1) Nodes are dimensional attributes <br />
            2) Edges describe n:1 associations between pairs of dimensional
            attributes. NB: there can be <a href="#multiple_edges">exceptions</a
            ><br />
            3) Root is the considered dimension
          </p>
          <h5>Example</h5>
          <p>
            A hierarchy can be: date(root) -> month -> trimester -> year. <br />
            NB: it is also possible that from date there are other paths. E.g.
            date -> holiday or date -> week
          </p>
        </li>
        <li>
          <h4>Primary event</h4>
          <p>
            A primary event is an occurrence of a fact; it is represented by
            means of a tuple of values.
          </p>
          <h5>Example</h5>
          <p>
            On 10/10/2001, ten ‘Brillo’ detergent packets were sold at the
            BigShop for a total amount of 25 euros.
          </p>
        </li>
        <li>
          <h4>Secondary event</h4>
          <p>
            A secondary event aggregates some of the corresponding primary
            events.
          </p>
          <h5>Example</h5>
          <p>
            Sales can be grouped by Product and Month: <br />
            In October 2001, 230 ‘Brillo’ detergent packets were sold at the
            BigShop for a total amount of 575 euros. <br />
            NB: The sales can be further grouped by Product, Month, and City
          </p>
        </li>
        <li>
          <h4 id="descriptive_attribute">Descriptive attribute</h4>
          <p>
            1) A descriptive attribute contains additional information about a
            dimensional attribute <br />
            2) They are uniquely determined by the corresponding dimensional
            attribute 3) Represented by edges only connected to the dimension
          </p>
        </li>
        <li>
          <h4>Optional edges</h4>
          <p>
            Some edges of a fact schema could be optional. <br />
            Represented by a ortogonal segment onto the edge
          </p>
          <h5>Example</h5>
          <p>
            The "Diet" edge of a sale fact only assumes a value if the specific
            item is food.
          </p>
        </li>
        <li>
          <h4>Optional dimensions</h4>
          <p>
            Some dimensions of a fact schema could be optional <br />
            Represented by a ortogonal segment onto the edge connecting the
            dimension to the fact
          </p>
          <h5>Example</h5>
          <p>
            The attribute Promotion amount of a sale fact only assumes a value
            for products in promotion, therefore only products in promotion can
            be analysed along that dimension
          </p>
        </li>
        <li>
          <h4 id="cross_dimensional_attributes">
            Cross dimensional attributes
          </h4>
          <p>
            A cross-dimensional attribute is a dimensional or a descriptive
            attribute whose value is obtained by combining values of some
            dimensional attributes
          </p>
          <h5>Example</h5>
          <p>
            For example, IVA (VAT) is computed based on the product category and
            the state.
          </p>
        </li>
        <li>
          <h4 id="convergence">Convergence</h4>
          <p>
            We have a convergence when 2 dimensional attributes are connected by
            more than two distinct directed edges.
          </p>
          <h5>Example</h5>
          <p>
            Shop -> city -> county -> state <br />
            Shop -> sale district -> state
          </p>
        </li>
        <li>
          <h4 id="hierarchy_sharing">Hierarchy sharing</h4>
          <p>
            1) In a fact schema, some portions of a hierarchy might be
            duplicated <br />
            2) As a shorthand we allow hierarchy sharing <br />
            3) If the sharing starts with a dimension attribute, it is necessary
            to indicate the roles on the incoming edges <br />
            4) Necessary condition: the functional dependency must hold on both
            branches <br />
            5) Represented by an arrow from the fact(s) that use the shared
            hierarchy to the hierarchy itself.
          </p>
        </li>
        <li>
          <h4 id="#multiple_edges">Multiple Edges</h4>
          <p>
            Some attributes, or some dimensions, may be related by a
            many-to-many relationship, we denote them by multiple edges. NB: at
            logical design time, they are dealt with in a special way!
          </p>
          <h5>Example</h5>
          <p>
            A book may have multiple authors and an author might have written
            multiple books.
          </p>
        </li>
        <li>
          <h4>Measure Aggregation</h4>
          <p>
            Aggregation requires to specify an operator to combine the values
            related to primary events into a unique value related to a secondary
            event. <br />
            Represented by a dotted edge between the measure and the dimension,
            where the possible aggregation operations are specified.
          </p>
          <h5>Example</h5>
          <p>
            Given a fact "inventory", its measure Q.ty in stock (so-called level
            measure) is non-additive w.r.t. the time dimension, but it is
            aggregable by means of the AVG and MIN operators. <br />
            Explained: summing the quantities in stock over a certain period of
            time makes no sense, computing minimum and average does.
          </p>
        </li>
        <li>
          <h4>Empty facts</h4>
          <p>
            A fact schema is empty if there are no measures. In fact, the
            default measure is the count.
          </p>
        </li>
        <li>
          <h4>Conceptual design</h4>
          <p>
            Conceptual design takes into account the documentation related to
            the integrated database <br />
            1) Conceptual schema (E/R, UML class diagr.,….) <br />
            2) Logical schema (e.g. relational, XML… )
          </p>
        </li>
        <li>
          <h4>Top-down methodology</h4>
          <p>
            1) Fact choice (subject oriented !!) <br />
            2) For each fact: <br />
            2.1) Design of the attribute tree <br />
            2.2) Attribute-tree editing <br />
            2.3) Dimension definition <br />
            2.4) Measure definition <br />
            2.5) Fact schema creation
          </p>
        </li>
        <li>
          <h4>Fact definition</h4>
          <p>
            Facts correspond to events that dynamically happen in the
            organization <br />
            1) In an E/R schema, it can correspond to an entity F or to a
            relationship among n entities E1, E2, …, En <br />
            2) In a relational schema, often a fact corresponds to a relation
            (table) R <br />
            3) Good fact candidates: entities or relationships representing
            frequently updated archives, e.g. “sales”. Static archives: NO!.
            <br />
            4)When we identify a fact to be studied, it becomes the root of the
            fact schema
          </p>
        </li>
        <li>
          <h4>Attribute tree definition</h4>
          <p>
            1) The attribute tree is composed by: <br />
            1.1) Nodes, corresponding to attributes (simple or complex) of the
            source schema <br />
            1.2) Root, corresponding to the primary key of the fact we analyze
            <br />
            1.3) For each node, the corresponding attribute functionally
            determines its descendant attributes <br />
            2 The attribute tree can be obtained from the ER schema by using a
            semi-automatic procedure
          </p>
        </li>
        <li>
          <h4>Attribute tree editing</h4>
          <p>
            The editing phase allows to remove some attributes which are
            irrelevant for the data mart <br />
            1) Pruning of a node v: the subtree rooted in v is deleted <br />
            2) Grafting of a node v: the children of v are directly connected to
            the father of v
          </p>
          <h5>Special cases</h5>
          <p>
            1) Cyclic relationships (e.g. part-subpart, employee-manager) must
            be broken after a certain number of iterations <br />
            2) Cycles in the schema must be broken, possibly choosing to keep
            the most convenient link <br />
            3) ISA hierarchies (hereditarietarity) must be treated like optional
            (0:1) relationships <br />
            4) Compound attribute: vertex with children
          </p>
        </li>
        <li>
          <h4>Dimension definition</h4>
          <p>
            1) Dimensions can be chosen among the attributes that are children
            of the root of the tree <br />
            2) Time should always be a dimension <br />
            2.1) Historical source: time is an attribute <br />
            2.2) Snapshot source: not always time is directly represented. In
            this case it is necessary to add time.
          </p>
        </li>
        <li>
          <h4 id="measure">Measure definition</h4>
          <p>
            1) While the set of attributes that acts as fact identifier is
            included in the set of dimensions, numerical attributes of the root
            (fact) are measures (e.g., unitary price) <br />
            2) More measures are defined by applying aggregate functions to
            other attributes of the tree (e.g. ticket count) <br />
            3) Typical aggregate functions: sum, average, min, max, count <br />
            4) It is possible that a fact has no measures (empty) <br />
            5) If the granularity of a fact is different w.r.t. the granularity
            of the source schema, it can be useful to define suitable measures
            in order to aggregate the same attribute by using various operators
          </p>
        </li>
        <li>
          <h4>Glossary</h4>
          <p>
            In the glossary, an expression is associated with each measure. The
            expression describes how we obtain the measure at the different
            levels of aggregation starting from the attributes of the source
            schema
          </p>
        </li>
        <li>
          <h4>Fact schema creation</h4>
          <p>
            The attribute tree is translated into a fact schema including
            dimensions and measures <br />
            1) Dimension hierarchies correspond to subtrees having as roots the
            different dimensions (with the least granularity) <br />
            2) The fact name corresponds to the name of the selected entity
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>DW Logical Design</h3>
      <ul>
        <li>
          <h4>Data Mart logical models</h4>
          <p>
            1) MOLAP (Multidimensional On-Line Analytical Processing) stores
            data by using a multidimensional data structure <br />
            2) ROLAP (Relational On-Line Analytical Processing) uses the
            relational data model to represent multidimensional data
          </p>
        </li>
        <li>
          <h4>MOLAP</h4>
          <p>
            MOLAP (Multidimensional OLAP) is the more traditional way of OLAP
            analysis. Data is natively stored in a multidimensional cube. The
            storage is not in the relational database, but in proprietary
            formats.
          </p>
          <h5>Advantages</h5>
          <p>
            Excellent performance: MOLAP cubes are built for fast data
            retrieval, and are optimal for slicing and dicing operations. Can
            perform complex calculations: All calculations have been
            pre-generated when the cube is created. Hence, complex calculations
            are not only doable, but they return quickly.
          </p>
          <h5>Disadvantages</h5>
          <p>
            Limited in the amount of data it can handle: Because all
            calculations are performed when the cube is built, it is not
            possible to include a large amount of data in the cube itself. This
            is not to say that the data in the cube cannot be derived from a
            large amount of data. Indeed, this is possible. But in this case,
            only summary-level information will be included in the cube itself.
            Requires additional investment: Cube technology are often
            proprietary and do not already exist in the organization. Therefore,
            to adopt MOLAP technology, chances are additional investments in
            human and capital resources are needed.
          </p>
        </li>
        <li>
          <h4 id="rolap">ROLAP</h4>
          <p>
            ROLAP (Relational OLAP) relies on manipulating the data stored in
            the relational database to give the appearance of traditional OLAP's
            slicing and dicing functionality. In essence, each action of slicing
            and dicing is equivalent to adding a "WHERE" clause in the SQL
            statement.
          </p>
          <h5>Advantages</h5>
          <p>
            Can handle large amounts of data: The data size limitation of ROLAP
            technology is the limitation on data size of the underlying
            relational database. Can leverage functionalities inherent in the
            relational databases.
          </p>
          <h5>Disadvantages</h5>
          <p>
            Performance can be slow: Because each ROLAP report is essentially a
            SQL query (or multiple SQL queries) in the relational database, the
            query time can be long if the underlying data size is large. Limited
            by SQL functionalities: Because ROLAP technology mainly relies on
            generating SQL statements to query the relational database, and SQL
            statements do not fit all needs (for example, it is difficult to
            perform complex calculations using SQL), ROLAP technologies are
            therefore traditionally limited by what SQL can do. ROLAP vendors
            have mitigated this risk by building into the tool out-of-the-box
            complex functions as well as the ability to allow users to define
            their own functions.
          </p>
        </li>
        <li>
          <h4>HOLAP</h4>
          <p>
            HOLAP (Hybrid OLAP) attempts to combine the advantages of MOLAP and
            ROLAP. For summary-type information, HOLAP leverages cube technology
            for faster performance. When detail information is needed, HOLAP can
            "drill through" from the cube into the underlying relational data.
          </p>
        </li>
        <li>
          <h4>Star Schema (<a href="#rolap">ROLAP</a>)</h4>
          <p>
            A star schema is: <br />
            1) A set of relations DT1, DT2, …DTn - dimension tables each
            corresponding to a dimension. <br />
            2) Each relation DTi is characterized by a primary key di and by a
            set of attributes describing the analysis dimensions with different
            aggregation levels. <br />
            3) A relation FT, fact table, that imports the primary keys of
            dimensions tables. The primary key of FT is d1 d2 … dn ; FT contains
            also an attribute for each measure. <br />
            NB: It is possible to define different variants of the star schema
            to manage aggregate data, e.g. in a unique fact table
          </p>
          <h5>Considerations</h5>
          <p>
            1) The fact table contains information expressed at different
            aggregation levels <br />
            2) Dimension table keys are surrogates (i.e. generated ids), for
            space efficiency reasons <br />
            3) Dimension tables are de-normalized: note that product-> type ->
            category is a transitive dependency <br />
            4) De-normalization introduces redundancy, but fewer joins to do
          </p>
          <h5>OLAP queries on Star Schema</h5>
          <p>
            select City, Week, Type, sum(Quantity) <br />
            from Week, Shop, Product, Sale <br />
            where Week.ID_Week=Sale.ID_Week and Shop.ID_Shop=Sale.ID_Shop and
            Product.ID_Product=Sale.ID_Product and Product.Category =
            ‘FoodStuff’ <br />
            group by City,Week,Type
          </p>
        </li>
        <li>
          <h4>Snowflake Schema (<a href="#rolap">ROLAP</a>)</h4>
          <p>
            1) The snowflake schema reduces the denormalization of the
            dimensional tables DTi of a star schema, removing some transitive
            dependencies. <br />
            2) Dimensions tables of a snowflake schema are composed by: <br />
            2.1) a primary key di,j <br />
            2.2) a subset of DTi attributes that directly depend on di,j <br />
            2.3) zero or more external keys that allow to obtain the entire
            information <br />
            3) The snowflake schema has 2 types of dimension tables:
            <br />
            3.1) Primary dimension tables: their keys are imported in the fact
            table <br />
            3.2) Secondary dimension tables
          </p>
          <h5>Considerations</h5>
          <p>
            1) Reduction of memory space <br />
            2) New surrogate keys <br />
            3) Advantages in the execution of queries related to attributes
            contained into fact and primary dimension tables
          </p>
          <h5>Normalization</h5>
          <p>
            If there exists a cascade of transitive dependencies, attributes
            depending (transitively or not) on the snowflake attribute are
            placed in a new relation.
          </p>
          <h5>OLAP queries on snowflake schema</h5>
          <p>
            select City, Week, Type, sum(Quantity) <br />
            from Week, Shop, Type, City, Product, Sale where
            Week.ID_Week=Sale.ID_Week and Shop.ID_Shop=Sale.ID_Shop and
            Shop.ID_City = City.ID_City and Product.ID_Product=Sale.ID_Product
            and Product.ID_Type=Type.ID_Type and Product.Category = ‘FoodStuff’
            <br />
            group by City,Week, Type
          </p>
        </li>
        <li>
          <h4>Views</h4>
          <p>
            1) A view denotes a fact table containing aggregate data. They are
            defined in order to pre-computate the aggregation. <br />
            2) A view can be characterized by its aggregation level (pattern)
            <br />
            2.1) Primary views: correspond to the primary aggregation levels
            <br />
            2.2) Secondary views: correspond to secondary aggregation levels
            (secondary events)
          </p>
        </li>
        <li>
          <h4>Derived measures</h4>
          <p>
            Derived measures are measures obtained by applying mathematical
            operators to two or more values of the same tuple. They are useful
            in order to manage aggregations correctly.
          </p>
        </li>
        <li>
          <h4>Aggregate operators</h4>
          <p>
            1) Distributive operator: allows to aggregate data starting from
            partially aggregated data (e.g. sum, max, min) <br />
            2) Algebraic operator: requires further information to aggregate
            data (e.g. avg) <br />
            3) Holistic operator: it is not possible to obtain aggregate data
            starting from partially aggregate data (e.g. mode, median) <br />
            4) Currently, aggregate navigators are included in the commercial DW
            system <br />
            5) They allow to re-formulate OLAP queries on the “best” view <br />
            6) They manage aggregates only by means of distributive operators
          </p>
        </li>
        <li>
          <h4>Aggregate data in the standard star schema</h4>
          <p>
            1) First solution (already seen): data of primary and secondary
            views are stored in the same fact table, which means NULL values for
            attributes having aggregation levels finer than the current one
            <br />
            2) Second solution: distinct aggregation patterns are stored in
            distinct fact tables: constellation schema <br />
            2.1) Only the dimension of the fact table is reduced, but this is a
            great improvement <br />
            2.2) Max optimization level: separate fact tables, and also
            replicate dimension tables for different (the most used) aggregation
            levels
          </p>
        </li>
        <li>
          <h4>Logical modelling (ROLAP)</h4>
          <p>
            Sequence of steps that, starting from the conceptual schema, allow
            one to obtain the logical schema for a specific data mart. <br />
            1) Choice of the logical schema (star/snowflake schema) <br />
            2) Conceptual schema translation <br />
            3) Choice of the materialized views <br />
            4) Optimization
          </p>
        </li>
        <li>
          <h4>Workload</h4>
          <p>
            1) In OLAP systems, workload is dynamic in nature and intrinsically
            extemporaneous. <br />
            2) During requirement collection phase, it must be deducted (e.g.
            from user feedback, reports) <br />
            Characterize OLAP operations: <br />
            2.1) Based on the required aggregation pattern <br />
            2.2) Based on the required measures <br />
            2.3) Based on the selection clauses <br />
            3) At system run-time, workload can be desumed from the system log
          </p>
        </li>
        <li>
          <h4>Data volume</h4>
          <p>
            Depends on: 1) Number of distinct values for each attribute <br />
            2) Attribute size <br />
            3) Number of events (primary and secondary) for each fact <br />
            Determines: <br />
            1) Table dimension <br />
            2) Index dimension <br />
            3) Access time
          </p>
        </li>
        <li>
          <h4>From fact schema to star schema</h4>
          <p>
            1) Create a fact table containing measures and descriptive
            attributes directly connected to the fact <br />
            2) For each hierarchy, create a dimension table containing all the
            attributes
          </p>
        </li>
        <li>
          <h4>Descriptive attributes</h4>
          <p>
            1) If it is connected to a dimensional attribute, it has to be
            included in the dimension table containing the attribute (see next
            slide, snowflake example, agent) <br />
            2) If it is connected to a fact, it has to be directly included in
            the fact schema
          </p>
        </li>
        <li>
          <h4>Optional attributes</h4>
          <p>Introduction of null values or ad-hoc values</p>
        </li>
        <li>
          <h4>Cross dimensional attributes</h4>
          <p>
            1) A cross-dimensional attribute b defines an N:M association
            between two or more dimensional attributes a1,a2, …, ak <br />
            2) It requires to create a new table including b and having as key
            the attributes a1,a2, …, ak
          </p>
        </li>
        <li>
          <h4>Shared hierarchies and convergence</h4>
          <p>
            1) A shared hierarchy is a hierarchy which refers to different
            elements of the fact table (e.g. caller number, called number)
            <br />
            2) The dimension table should not be duplicated <br />
            3) Two different situations: <br />
            3.1) The two hierarchies contain the same attributes, but with
            different meanings (e.g. phone call à caller number, phone call à
            called number) <br />
            3.2) The two hierarchies contain the same attributes only for part
            of the hierarchy trees. Here we could also decide to replicate the
            shared portion
          </p>
        </li>
        <li>
          <h4>Multiple edges (V1)</h4>
          <p>
            A bridge table models the multiple edge: the key of the bridge table
            is composed by the combination of attributes connected to the
            multiple edge and the weight column. The weight of the edge is the
            contribution of each edge to the cumulative relationship. <br />
            1) Weighed queries take into account the weight of the edge. <br />
            2) Impact queries do not take into account the weight of the edge
          </p>
          <h5>Example of weighed query</h5>
          <p>
            Profit for each author: <br />
            SELECT AUTHORS.Author,SUM(SALES.Profit * BRIDGE.Weight) <br />
            FROM AUTHORS, BRIDGE, BOOKS, SALES <br />
            WHERE AUTHORS.Author_id=BRIDGE.Author_id AND
            BRIDGE.Book_id=BOOKS.Book_id AND BOOKS.Book_id=SALES.Book_id <br />
            GROUP BY AUTHORS.Author
          </p>
          <h5>Example of impact query</h5>
          <p>
            Sold copies for each author: <br />
            SELECT AUTHORS.Author, SUM(SALES.Quantity) <br />
            FROM AUTHORS, BRIDGE, BOOKS, SALES <br />
            WHERE AUTHORS.Author_id=BRIDGE.Author_id AND
            BRIDGE.Book_id=BOOKS.Book_id AND BOOKS.Book_id=SALES.Book_id <br />
            GROUP BY AUTHORS.Author
          </p>
        </li>
        <li>
          <h4>Multiple edges (V2)</h4>
          <p>
            If we want to keep the star model, we can connect both the nodes of
            the edge to the fact, effectively making the farthest node anoder
            dimansion.
          </p>
          <h5>Example</h5>
          <p>
            sales -> books -> authors where books -> authors in a multile edge.
            <br />
            add authors to the fact schema of sales (as another dimension!!).
            <br />
            NB: Here we don’t need the weight because the fact table records
            quantity and profit per book and per author
          </p>
        </li>
        <li>
          <h4>Secondary view precomputation</h4>
          <p>
            The choice about views that have to be materialized takes into
            account contrasting requirements: <br />
            1) Cost functions’ minimization: Workload cost vs View maintenance
            cost <br />
            2) System constraints: Disk space vs Time for data update <br />
            3) Users constraints: Max answer time vs Data freshness
          </p>
        </li>
        <li>
          <h4>Materialized views</h4>
          <p>
            1) It is useful to materialize a view when it directly solves a
            frequent query and/or it reduce the costs of some queries <br />
            2) It is not useful to materialize a view when its aggregation
            pattern is the same as another materialized view and/or its
            materialization does not reduce the cost
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>Tips for DW</h3>
      <ul>
        <li>
          <h4>Reverse engineering</h4>
          <p>Should be easy</p>
        </li>
        <li>
          <h4>Conceptual design</h4>
          <p>
            1) Find the tables of which to draw the attribute tree, usually the
            first nomined table <br />
            2) Draw the attribute tree (easy). If the relation is N:1 or N:N you
            cannot put that attribute. (e.g. If a Policy can have multiple
            Accidents, accidents cannot be amìn attribute of policy)<br />
            2.1) Do grafting when the Primary Key is not used, do Pruning on any
            unused attributes. <br />
            3) Underline the used attributes and start from them to build the
            fact schema <br />
            4) Remember to take into consideration the queries when writing the
            <a href="#measure">measures</a> <br />
            5) The glossary is the description of the attributes inside the fact
            table. NB: the "count" attribute is the default and always present,
            even if not used.<br />
            6) Build the glossary as queries on the "fact" table, also other
            tables if needed, note that you can only access directly connected
            attributes. (e.g. from a "Accident" fact you can only access the
            date attribute (even if date has multiple granularities)) <br />
            7) In the glossary query select always all the possible dimensions
            and Group By the finest grain possible (basically group by
            everything except the aggregated value)
          </p>
        </li>
        <li>
          <h4>Logical Design</h4>
          <p>
            1) Draw the tables associated with the fact schemas drawn in the
            Conceptual Design. <br />
            2) Find any common hyerarchies and draw them as table (e.g. Date
            hyerarchy can be used by multiple tables)<br />
            3) Find any not final nodes and draw them as tables. <br />
            NB: Obviously pruned or grafted attributes are NOT included.
          </p>
        </li>
        <li>
          <h4>Query answering</h4>
          <p>
            Just answer the queries using the Logical Design you built as
            reference.
          </p>
        </li>
      </ul>
    </section>
    <section>
      <h3>Trends in data integration</h3>
      <ul>
        <li>
          <h4>Uncertain Data</h4>
          <p>
            Databases are assumed to represent certain data: a tuple in the
            database is true, any tuple NOT in the database is false (Closed
            World Assumption). But real life is not as certain!. Hence there are
            uncertain databases of various kinds (e.g. based on probability)
            attempt to model uncertain data and to answer queries in an
            uncertain world
          </p>
        </li>
        <li>
          <h4>Uncertainty in Data Integration</h4>
          <p>
            1) Data itself may be uncertain (e.g. extracted from an unreliable
            source) <br />
            2) Mappings might be approximate (e.g. created by relying on
            automatic ontology matching) <br />
            3) Reconciliation is approximate <br />
            4) Approximate mediated schema <br />
            5) Imprecise queries, such as keyword-search queries, are
            approximate
          </p>
        </li>
        <li>
          <h4>Uncertain Databases</h4>
          <p>
            An uncertain database describes a set of possible worlds. An
            uncertain database might assign each tuple a probability. Then the
            probability of a possible world is the product of the probabilities
            for the tuples. However, this does not represent possible
            correlations between tuples.
          </p>
        </li>
        <li>
          <h4>Probabilistic mediated schema</h4>
          <p>
            1) first construct the multiple mediated schemas Med1, . . . , Medl
            in pMed, and then assign each of them a probability. <br />
            2) Two pieces of information available in the source schemas can
            serve as evidence for attribute clustering: <br />
            2.1) pairwise similarity of source attributes: indicates when two
            attributes are likely to be similar, and is used for creating
            multiple mediated schemas <br />
            2.2) statistical co-occurrence properties of source attributes:
            indicates when two attributes are likely to be different, and is
            used for assigning probabilities to each of the mediated schemas
          </p>
        </li>
        <li>
          <h4>Trends in <a href="#record_linkage">entity resolution</a></h4>
          <p>
            1) A very common approach: use of record-matching rules (rule-based
            ER), i.e., constraints that state: “if any two records are similar
            on properties P1,...Pn, then they refer to the same entity".
            Problems: <br />
            1.1) it is difficult to generate record-matching rules; <br />
            1.2) it is challenging and time-costly to define the bounds of
            similarity for approximate-match conditions; <br />
            1.3) one-to-one record matching too expensive (quadratic). Possible
            solution: blocking <br />
            2) Data Fusion, many possible methods, e.g., source authority: refer
            to the most reliable sources to decide which is the right one among
            many different values, but:
          </p>
        </li>
        <li>
          <h4>Data Provenance</h4>
          <p>
            Sometimes knowing where the data have come from and when and how
            they were produced is critical. <br />
            The database community models provenance in terms of how the datum
            was derived from the original source databases, the rest is left to
            the application (it is assumed to be domain dependent)
          </p>
          <h5>Two Viewpoints on Provenance</h5>
          <p>
            1) Provenance as Annotations on Data: models provenance as a series
            of annotations describing how each data item was produced. These
            annotations can be associated with tuples or values <br />
            2) Provenance as a Graph of Data Relationships: it models provenance
            as a (hyper)graph, with tuples as vertices. Each possible direct
            derivation of a tuple from a set of source tuples is a hyper-edge
            connecting the source and derived tuples <br />
            The two views are equivalent, and it is possible to convert one into
            the other as convenient
          </p>
        </li>
        <li>
          <h4>Crowdsourcing</h4>
          <p>
            Some checks are very simple for humans but hard for a computer.
            <br />
            Can provide powerful solutions to traditionally hard data
            integration problems (e.g. wrapping, as above, check correcteness of
            schema mappings, etc.)
          </p>
        </li>
        <li>
          <h4>Building Large-Scale Structured Web Databases</h4>
          <p>
            New challenge besides the ones we have discussed: develop
            methodologies for designing a fully integrated database coming from
            heterogeneous data sources: <br />
            1) Do we clean each source first (first acting on the data), then
            merge them? <br />
            2) Do we merge the source schemas (when they are present) and then
            clean the data? <br />
            3) How do we manage the IDs of the identified entities, and how do
            we navigate through them? We have no more a uniform query language
            to access and manipulate well-identified objects! <br />
            4) Incremental source update and use human feedback during the
            update process. <br />
          </p>
        </li>
        <li>
          <h4>Pay-as-You-Go Data Management</h4>
          <p>
            1) Data integration on the Web is an extreme example motivating
            pay-as-you-go data management. <br />
            2) In contrast to the other kinds of data integration systems,
            pay-as-you-go systems try to avoid the need for the initial setup
            phase that includes creating the mediated schema and source
            descriptions. <br />
            3) The goal is to offer a system that provides useful services on a
            collection of heterogeneous data with very little up-front effort.
            <br />
          </p>
        </li>
        <li>
          <h4>dataspaces</h4>
          <p>
            Two basic principles: <br />
            1) keyword search over a collection of data coupled with effective
            data visualization. This can be enriched with some of the techniques
            for automatic schema or instance matching, automatic extraction of
            metadata. <br />
            2) Improving the metadata in the system to the end of supporting and
            validating schema mappings, instance and reference reconciliation,
            or improve the results of information extraction.
          </p>
        </li>
        <li>
          <h4>Data Spaces in practice: Data Lakes</h4>
          <p>
            Data lake technologies are promising solutions for enhancing data
            management and analysis capabilities: <br />
            1) Managing big-data volume and variety, <br />
            2) Providing data analysts with a self-service environment in which
            advanced analytics can be applied.
          </p>
          <h5>Requirements</h5>
          <p>
            1) Connectivity <br />
            2) Integration of heterogeneous data <br />
            3) Data-storage capacity <br />
            4) Appropriate solutions to ingest, store, and process the enormous
            amount of information coming from all the involved, typically
            heterogeneous data sources. <br />
            5) Possibility to support Data-analysis capabilities (Statistics,
            Data Mining, Machine Learning)
          </p>
          <h5>Research challenges</h5>
          <p>
            1) document data quality, <br />
            2) facilitate data discovery, <br />
            3) define security and privacy policies
          </p>
          <h5>Data Lake Minimal Toolset</h5>
          Given the previous requirements, and the typically related research
          challenges, we need tools for:
          <p>
            1) data ingestion, to load data from the data sources, depending on
            the nature of the source, in a batch or stream mode. <br />
            2) data governance that, among the others, include the tools for
            data quality and the data catalog. <br />
            3) data storage that includes specific solutions for storing
            sensitive data. <br />
            4) data access, to enable the final users – such as business
            analysts or data scientists – to search and obtain the data they
            desire.
          </p>
        </li>
        <li>
          <h4>Research Challenges</h4>
          <p>
            1) A suitable infrastructure for fast and controlled ingestion of
            data, and subsequent storage; <br />
            2) Models and tools to support data reconciliation and integration;
            <br />
            3) A data catalogue for describing sources in such a way as to
            facilitate their search, exploration, and integration; <br />
            4) Security and privacy requirements’ management policies
          </p>
        </li>
        <li>
          <h4>Querying a traditional integration system</h4>
          <p>
            When a query or an update is submitted to a traditional integration
            system, this has to perform some reformulation in terms of a set of
            queries over the single datasets: <br />
            1) The system determines which datasets are needed to answer the
            query <br />
            2) If more than one dataset is needed, determine which predicates
            (conditions) apply to only a single dataset and which predicates
            apply to elements from more than one dataset. <br />
            3) The latter ones can only be evaluated over all the involved
            datasets, appropriately combined, whereas each of the former can be
            evaluated over the specific dataset addressed by that (sub) query.
          </p>
        </li>
        <li>
          <h4>Pay-as-you-go approach in the data lake(s)</h4>
          <p>
            In a data lake these operations are performed in a pay-as-you-go
            fashion, avoiding the creation of mediated schemas or the permanent
            establishment of relationships between the elements of different
            sources (entity resolution and data fusion), and relying instead on
            temporary links based on metadata. Basic tools: <br />
            1) keyword search over a collection of data, possibly coupled with
            effective data visualization <br />
            2) improving the metadata in the system to the end of supporting and
            validating schema and instance, instance and reference
            reconciliation
          </p>
        </li>
        <li>
          <h4>Comparing data in data lakes</h4>
          <p>
            1) The integration of natural language text can be supported by
            semantic techniques (ontologies are numerous and rich in the health
            domain) and/or on ML tools ( e.g. word and sentence embeddings)<br />2)
            Queries that require to retrieve images, or time series, that show
            some resemblance, might be based on keyword-search on the image
            features or other metadata, or exploit some service based on image
            or plot similarity<br />
            3) The data lake should be able to establish a relationship between
            datasets that have different characteristics, and also allow
            different modes of interaction<br />
            4) Fundamental a data catalog for describing data and services<br />
            5) This information is the basis to allow search, query and update
            over all the contents of the data lake, regardless of their formats
          </p>
        </li>
        <li>
          <h4>Data Lake design</h4>
          <p>
            Data Lake can be designed on premises or in cloud, Data Lake can be
            realized also as a virtual data lake (ingest on demand).
          </p>
        </li>
        <li>
          <h4>Main tasks of the data catalog</h4>
          <p>
            1) Define tags for attributes and sources using consistent terms
            <br />
            2) Provide an interface that allows users to find data sources using
            terms they are accustomed to <br />
            3) Provide a description of the content of the sources through
            appropriate tags <br />
            4) Include details about the provenance of the data (e.g., where it
            came from, whether and what preprocessing operations were performed
            on it)
          </p>
        </li>
        <li>
          <h4>Metadata</h4>
          <p>
            1) Descriptive metadata add information about who created a
            resource, what it is about and what it includes. This is best
            applied using semantic annotations. <br />
            2) Structural metadata include data about the way data elements are
            organized, their relationships and the (possible) structure they
            exist in. <br />
            3) Administrative metadata provide information about the origin of
            resources, their type and access rights. <br />
            4) Semantic metadata to interpret the meaning of the data via
            references to concepts, often formally described in a knowledge
            graph or ontology. <br />
            A different, more coarse-grained classification considers: <br />
            1) Technical Metadata, related to the profile of the datasets (e.g.,
            list of attributes, data types, statical information), and <br />
            2) Business Metadata, that provide a domain-aware description of a
            dataset.
          </p>
        </li>
        <li>
          <h4>Retrieving Technical metadata</h4>
          <p>
            Data Profiling: the activity that extracts metadata from data
            analysis. Data profiling highlights the schema, data types, formats
            used, maximum, minimum, average (if numeric) values, and other
            information such as: Cardinality (number of unique values per
            attribute), Selectivity (a measure of the uniqueness of values,
            measured by dividing cardinality by the number of rows), Density
            (number of nulls per attribute).
          </p>
        </li>
        <li>
          <h4>Retrieving Business metadata</h4>
          <p>
            1) Business metadata are often derived from glossaries, taxonomies
            and ontologies <br />
            2) Business ontologies are often very complex. That is why in their
            place many times so-called folksonomies are used: user-built
            ontologies in which recurring terms are usually used and classified.
          </p>
        </li>
        <li>
          <h4>Tagging</h4>
          <p>
            Tagging is the process by which metadata is associated with data
            sources. Tagging can be: Manual, Crowdsource-based, Automatic
          </p>
        </li>
        <li>
          <h4>Other relevant metadata</h4>
          <p>
            Metadata related to security and data protection, Data Quality and
            Data Lineage (or Provenance)
          </p>
        </li>
        <li>
          <h4>Data lake federation</h4>
          <p>
            Adoption of a data lake federation through which the organizations
            could achieve further benefits by sharing data. <br />
            The main research challenge for guaranteeing data reliability is
            accurate data description, to: document their quality, facilitate
            data discovery, define security and privacy policies.
          </p>
          <h5 id="layers">Levels and layers</h5>
          <p>
            There are 2 levels of Data Lake Federation: Federation level
            (top-down) and Node Level (bottom-up). Theese 2 levels are then
            subdivided in 2 and 3 layers respectively. (for a total of 5)
          </p>
        </li>
        <li>
          <h4>Federation layer</h4>
          <p>
            it offers both infrastructural and application services: assuming
            that a set of nodes offers data services according to the structure
            just described, the data lake federation has the role to create an
            ecosystem that enables data sharing. For instance, a community cloud
            can be established to offer common storage and computation
            facilities that can be used by a node to (temporarily) store data in
            a convenient place to increase the performance. At the same time, a
            distributed monitoring system can be offered, to check if the
            occurring data-sharing is respecting the defined access policies.
          </p>
        </li>
        <li>
          <h4>Cooperation/community layer</h4>
          <p>
            it offers the tools to define and manage multi-centric operations,
            i.e., where the analysed data could be stored in more than one node.
          </p>
        </li>
        <li>
          <h4>Service layer (middle layer)</h4>
          <p>
            it implements the policies that regulate the access to the exposed
            datasets through the defined services. These policies specify who
            has the right to access what, and also which type of analysis can be
            performed on a given dataset or the transformations to be applied
            (e.g., anonymization) before making it available to the requester.
          </p>
        </li>
        <li>
          <h4>Service component layer</h4>
          <p>
            it supports the implementation of the data-access services, in
            charge to expose the selected datasets. For each service, depending
            on the nature of the data and the type of support that the node
            wants to offer, e.g., a simple FTP connection, a REST API.
          </p>
        </li>
        <li>
          <h4>Operational layer</h4>
          <p>
            organizations, companies, associates, ….have implemented their own
            data repositories to store their data. These might be deployed on
            the organization’s premises or on cloud resources, possibly already
            organized in a data lake or other data organization, or using a
            hybrid solution.
          </p>
        </li>
        <li>
          <h4>Data Mesh</h4>
          <p>
            A data mesh is a design strategy for enterprise data platform
            architectures, while a data lake is a (centralized or distributed)
            repository that stores data — structured and unstructured — in a raw
            format.
          </p>
        </li>
        <li>
          <h4>The principles of Data Meshs</h4>
          <p>
            1) Domain ownership: ownership shifts from the centralized ETL
            pipelines to decentralized units closer to the data following the
            principles of Domain-Driven Design <br />
            2) Data as a product: data teams must apply product thinking to the
            datasets provided ensuring discoverability and quality-control.
            Hence DATSIS principles are relevant: Discoverability,
            Addressability, Trustworthiness, Selfdescribing, Interoperability
            and Security <br />
            3) Self-serve data platform: Data product lifecycle should be
            supported by a self-serve data platform, providing the needed tools
            and infrastructures and accessible without specialized knowledge
            <br />
            4) Federated Computational Governance: Ensuring data ownership,
            standardization, interoperability and automated decision execution
          </p>
        </li>
        <li>
          <h4>Infrastructure</h4>
          <p>
            1) Depending on the types of data at stake, computational resources
            may require CPU or GPU support <br />
            2) Storage should be organized into a Hot Storage, containing
            frequently used data or data requiring quick access, and a cheaper
            and slower Cold Storage for less frequently used data <br />
            3) Data analysis can be performed both on-premises and on the cloud;
            <br />
            4) The network is used for ingestion and for querying: the former
            requires resources several orders of magnitude larger
          </p>
        </li>
        <li>
          <h4>Lightweight Integration</h4>
          <p>
            Many data integration tasks are transient: <br />
            1) We may need to integrate data from multiple sources to answer a
            question asked once or twice. The integration needs to be done
            quickly and by people without technical expertise (e.g. a disaster
            response situation in which reports are coming from multiple data
            sources in the field, and the goal is to corroborate them and
            quickly share them with the affected public) <br />
            2) Problems typical of lightweight data integration: locating
            relevant data sources, assessing source quality, helping the user
            understand the semantics, supporting the process of integration.
            <br />
            3) Ideally, machine learning and other techniques can be used to
            amplify the effects of human input, through semi-supervised
            learning, where small amounts of human data classification, plus
            large amounts of additional raw (“unlabeled”) data, are used to
            train the system.
          </p>
        </li>
        <li>
          <h4>Mashups: a paradigm for lightweight integration</h4>
          <p>
            The term mashup is widely used today: A mashup is an application
            that integrates two or more mashup components at any of the
            application layers (data, application logic, presentation layer)
            possibly putting them into communication with each other
          </p>
          <h5>Mashup component</h5>
          <p>
            Is any piece of data, application logic and/or user interface that
            can be reused and that is accessible either locally or remotely
            (e.g., Craigslist and Gmaps).
          </p>
          <h5>Mashup logic</h5>
          <p>
            Is the internal logic of operation of a mashup; it specifies the
            invocation of components, the control flow, the data flow, the data
            transformations, and the UI of the mashup
          </p>
          <h5>Other definitions of MashUps</h5>
          <p>
            1) “Web-based resources consisting of dynamic networks of
            interacting components” (Abiteboul et Al., 2008) <br />
            2) “API enablers” (Ogrinz, 2009), to create an own API where there
            is none <br />
            3) “Combination of content from more than one source into an
            integrated experience” (Yee, 2008)
          </p>
          <h5>Types of mashup</h5>
          <p>
            There are different types of mashup, even associated with different
            <a href="#layers">layers</a>. 1) Data mashups: Fetch data from
            different resources, process them, and return an integrated result
            set <br />
            2) Logic mashups: Integrate functionality published by logic or data
            components <br />
            3) User Interface (UI) mashups: Combine the component's native UIs
            into an integrated UI; the components’ UIs are possibly synchronized
            with each other <br />
            4) Hybrid mashups: Span multiple layers of the application stack,
            bringing together different types of components inside one and a
            same application;
          </p>
        </li>
        <li>
          <h4>Data mashup vs. data integration</h4>
          <p>
            Data mashups are a Web-based, lighweight form of data integration,
            meant to solve different problems. It handles both Application and
            Data integration in one simple step. Is should not be used in
            Transactional or Mission-Critical applications (traditional
            integration should be done in theese cases).
          </p>
        </li>
        <li>
          <h4>Visualizing Integrated Data</h4>
          <p>
            1) Visualize important patterns in the data instead of an infinite
            number of rows <br />
            2) Immediately see discrepancies between the data in the sources.
            <br />
            3) During the integration process, show a subset of the data that
            was not reconciled correctly <br />
            4) When browsing different collections of data to be integrated,
            visually show the search results and evaluate their relevance to the
            specific integration task <br />
            5) Visualization of the data provenance.
          </p>
        </li>
        <li>
          <h4>Integrating Social Media</h4>
          <p>
            1) Social media data are often noisy (spam and low-quality data)
            <br />
            2) Transient nature of such data and users: identifying quality data
            and influential users is difficult <br />
            3) The data lack context: hard to interpret <br />
            4) Data often arrive as high-speed streams that require very fast
            processing
          </p>
        </li>
        <li>
          <h4></h4>
        </li>
      </ul>
    </section>
    <h3>Exam Dates</h3>
    <ul class="types-list">
      <li>21 DEC</li>
      <li>22 JAN</li>
      <li>15 FEB</li>
      <!-- Add more exam dates as needed -->
    </ul>
  </body>

  <script>
    function toggleCompress(el) {
      console.log(el);
      if (!el.parentElement.classList.contains("compressed")) {
        el.parentElement.classList.add("compressed");
        return;
      }
      if (!el.parentElement.classList.contains("minimized")) {
        el.parentElement.classList.add("minimized");
        return;
      }
      el.parentElement.classList.remove("minimized");
      el.parentElement.classList.remove("compressed");
    }

    document
      .querySelectorAll("section h3")
      .forEach((el) => el.addEventListener("click", () => toggleCompress(el)));
    document
      .querySelectorAll("section")
      .forEach((el) => el.classList.add("compressed", "minimized"));
  </script>
</html>
]]
